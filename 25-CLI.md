# ollama cli

* cli talks with the server

```sh
$ ollama
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  signin      Sign in to ollama.com
  signout     Sign out from ollama.com
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
```

## Listing models

```
$ ollama ls
NAME                       ID              SIZE      MODIFIED
gemma3:270m                e7d36fb2c3b3    291 MB    49 minutes ago
gemma3:latest              a2af6cc3eb7f    3.3 GB    53 minutes ago
mistral-small:latest       8039dd90c113    14 GB     3 hours ago
nomic-embed-text:latest    0a109f422b47    274 MB    3 hours ago
mistral:latest             6577803aa9a0    4.4 GB    3 hours ago
smollm2:latest             cef4a1e09247    1.8 GB    3 hours ago
qwen2.5vl:latest           5ced39dfa4ba    6.0 GB    3 hours ago
smollm2:360m               297281b699fc    725 MB    3 hours ago
smollm2:135m               9077fe9d2ae1    270 MB    3 hours ago
qwen2.5:latest             845dbda0ea48    4.7 GB    3 hours ago
qwen2.5:1.5b               65ec06548149    986 MB    3 hours ago
qwen2.5:0.5b               a8b0c5157701    397 MB    3 hours ago
embeddinggemma:latest      85462619ee72    621 MB    3 hours ago
llama3.2:latest            a80c4f17acd5    2.0 GB    6 days ago

```

## Running a model

Run command will pull and drop you into an interactive chat.

```
$ ollama run gemma3:270m
```

To see some metrics, use the `--verbose` flag.

```
$ ollama run --verbose gemma3:270m
```


## Showing info

```
$ ollama show gemma3:270m

  Model
    architecture        gemma3
    parameters          268.10M
    context length      32768
    embedding length    640
    quantization        Q8_0

  Capabilities
    completion

  Parameters
    stop     "<end_of_turn>"
    top_k    64
    top_p    0.95

  License
    Gemma Terms of Use
    Last modified: February 21, 2024
    ...

```

Extended info available:

```
$ ollama show -h
Show information for a model

Usage:
  ollama show MODEL [flags]

Flags:
  -h, --help         help for show
      --license      Show license of a model
      --modelfile    Show Modelfile of a model
      --parameters   Show parameters of a model
      --system       Show system message of a model
      --template     Show template of a model
  -v, --verbose      Show detailed model information

Environment Variables:
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)
```

## Show modelfile

Modelfile wraps a model and parameters, it is a main customization option in ollama.

```
$ ollama show --modelfile gemma3:270m | head -30
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM gemma3:270m

FROM /usr/share/ollama/.ollama/models/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2
TEMPLATE """{{- $systemPromptAdded := false }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 }}
{{- if eq .Role "user" }}<start_of_turn>user
{{- if (and (not $systemPromptAdded) $.System) }}
{{- $systemPromptAdded = true }}
{{ $.System }}
{{ end }}
{{ .Content }}<end_of_turn>
{{ if $last }}<start_of_turn>model
{{ end }}
{{- else if eq .Role "assistant" }}<start_of_turn>model
{{ .Content }}{{ if not $last }}<end_of_turn>
{{ end }}
{{- end }}
{{- end }}"""
PARAMETER stop <end_of_turn>
PARAMETER top_k 64
PARAMETER top_p 0.95
LICENSE """Gemma Terms of Use

Last modified: February 21, 2024

By using, reproducing, modifying, distributing, performing or displaying any portion or element of Gemma, Model Derivatives including via any Hosted Service, (each as defined below) (collectively, the "Gemma Services") or otherwise accepting the terms of this Agreement, you agree to be bound by this Agreement.
```

* start from a file (gguf)
* parameters for token selection
* stop token
* there is no "system" prompt
* template is used to format parameters, e.g. a "system prompt" or previous messages

```
{{- $systemPromptAdded := false }}

{{- range $i, $_ := .Messages }}
    {{- $last := eq (len (slice $.Messages $i)) 1 }}
    {{- if eq .Role "user" }}<start_of_turn>user
        {{- if (and (not $systemPromptAdded) $.System) }}
            {{- $systemPromptAdded = true }}
            {{ $.System }}
        {{ end }}
        {{ .Content }}<end_of_turn>
        {{ if $last }}<start_of_turn>model
        {{ end }}
    {{- else if eq .Role "assistant" }}<start_of_turn>model
        {{ .Content }}
        {{ if not $last }}<end_of_turn>
        {{ end }}
    {{- end }}
{{- end }}
```

> These base models are then often “fine-tuned” for chat, which means training
> them on data that is formatted as a sequence of messages. The chat is still
> just a sequence of tokens, though! The list of role and content dictionaries
> that you pass to a chat model get converted to a token sequence, often with
> control tokens like `<|user|> or <|assistant|> or <|end_of_message|>, which
> allow the model to see the chat structure.

Two examples (mistral, zephyr):

```
<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you
today?</s> [INST] I'd like to show off how chat templating works! [/INST]

<|user|>\nHello, how are you?</s>\n<|assistant|>\nI'm doing great. How can I
help you today?</s>\n<|user|>\nI'd like to show off how chat templating
works!</s>\n
```

### Quick look: Prompt assembly

Prompt assembly for a generate request.


```go
    prompt := req.Prompt
    if !req.Raw {
        tmpl := m.Template
        if req.Template != "" {
            tmpl, err = template.Parse(req.Template)
            if err != nil {
                c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
                return
            }
        }

        var values template.Values
        if req.Suffix != "" {
            values.Prompt = prompt
            values.Suffix = req.Suffix
        } else {
            var msgs []api.Message
            if req.System != "" {
                msgs = append(msgs, api.Message{Role: "system", Content: req.System})
            } else if m.System != "" {
                msgs = append(msgs, api.Message{Role: "system", Content: m.System})
            }

            if req.Context == nil {
                msgs = append(msgs, m.Messages...)
            }

            for _, i := range images {
                imgPrompt := ""
                msgs = append(msgs, api.Message{Role: "user", Content: fmt.Sprintf("[img-%d]"+imgPrompt, i.ID)})
            }

            values.Messages = append(msgs, api.Message{Role: "user", Content: req.Prompt})
        }

        values.Think = req.Think != nil && req.Think.Bool()
        values.ThinkLevel = ""
        if req.Think != nil {
            values.ThinkLevel = req.Think.String()
        }
        values.IsThinkSet = req.Think != nil

        var b bytes.Buffer
        if req.Context != nil {
            slog.Warn("the context field is deprecated and will be removed in a future version of Ollama")
            s, err := r.Detokenize(c.Request.Context(), req.Context)
            if err != nil {
                c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
                return
            }
            b.WriteString(s)
        }

        if err := tmpl.Execute(&b, values); err != nil {
            c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
            return
        }

        prompt = b.String()
    }
```

We can show how it looks like with a debug flag:

```go
    // If debug mode is enabled, return the rendered template instead of calling the model
    if req.DebugRenderOnly {
        c.JSON(http.StatusOK, api.GenerateResponse{
            Model:     req.Model,
            CreatedAt: time.Now().UTC(),
            DebugInfo: &api.DebugInfo{
                RenderedTemplate: prompt,
                ImageCount:       len(images),
            },
        })
        return
    }
```

Let's use the API and curl for that:

```
$ curl -s $OLLAMA_HOST/api/generate -d '{ "model": "gemma3:270m", "prompt": "How are you today?", "_debug_render_only": true}'  | jq .
{
  "model": "gemma3:270m",
  "created_at": "2025-10-04T08:32:02.75059769Z",
  "response": "",
  "done": false,
  "_debug_info": {
    "rendered_template": "<start_of_turn>user\nHow are you today?<end_of_turn>\n<start_of_turn>model\n"
  }
}
```

Note, for an image model, the rendered template shows a placeholder for the image:

```
{
  "model": "qwen2.5vl",
  "created_at": "2025-10-04T08:50:13.496689609Z",
  "response": "",
  "done": false,
  "_debug_info": {
    "rendered_template": "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n[img-0]\n\nWhat is in this picture?<|im_end|>\n<|im_start|>assistant\n",
    "image_count": 1
  }
}
```

## ollama ps

* list running models ("loaded")

```
$ ollama ps
NAME           ID              SIZE      PROCESSOR    CONTEXT    UNTIL
gemma3:270m    e7d36fb2c3b3    531 MB    100% CPU     4096       4 minutes from now
```

Processor can be GPU as well.

```
$ ollama ps
NAME                ID              SIZE      PROCESSOR    CONTEXT    UNTIL
mistral:latest      6577803aa9a0    5.8 GB    100% GPU     4096       4 minutes from now
moondream:latest    55fc3abd3867    2.8 GB    100% GPU     4096       4 minutes from now
qwen2.5vl:latest    5ced39dfa4ba    8.0 GB    100% GPU     4096       3 minutes from now
```

Model are added, evicted by a scheduler.

## ollama serve

* usually put into a service

```
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=$PATH"
Environment="OLLAMA_HOST=0.0.0.0:11435"
Environment="OLLAMA_DEBUG=1"

[Install]
WantedBy=multi-user.target
```

## ollama pull

* pull model from registry or other source

## other subcommands

* signin, signout for publishing own models on platform, also: push
* rm for file cleanup
* cp and create for model customizations
