# Evaluation

Leaderboards, e.g.

* [https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro)

Example test:

* [Ollama-MMLU](https://github.com/chigkim/Ollama-MMLU-Pro/)

Adapted from [MMLU-Pro](https://github.com/TIGER-AI-Lab/MMLU-Pro)

> We introduce MMLU-Pro, an enhanced benchmark designed to evaluate language
> understanding models across broader and more challenging tasks. Building on
> the Massive Multitask Language Understanding (MMLU) dataset, MMLU-Pro
> integrates more challenging, reasoning-focused questions and increases the
> answer choices per question from four to ten, significantly raising the
> difficulty and reducing the chance of success through random guessing.
> MMLU-Pro comprises over 12,000 rigorously curated questions from academic
> exams and textbooks, spanning 14 diverse domains including Biology, Business,
> Chemistry, Computer Science, Economics, Engineering, Health, History, Law,
> Math, Philosophy, Physics, Psychology, and Others.

## Issues

* isolated tests
* training on test cases (for a random note, see various comments on r/locallama, e.g. [n6hg2rc](https://www.reddit.com/r/LocalLLaMA/comments/1mfitwb/comment/n6hg2rc/))
