Directory structure:
â””â”€â”€ ollamaintro/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ go.mod
    â”œâ”€â”€ go.sum
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Refs.md
    â”œâ”€â”€ Slides.md
    â”œâ”€â”€ attic/
    â”‚   â”œâ”€â”€ 10-Intro.md
    â”‚   â”œâ”€â”€ 20-Internals.md
    â”‚   â”œâ”€â”€ 21-Arch.md
    â”‚   â”œâ”€â”€ 22-GGUF.md
    â”‚   â”œâ”€â”€ 25-Eval.md
    â”‚   â”œâ”€â”€ 25-Tokenization.md
    â”‚   â”œâ”€â”€ 30-Customizations.md
    â”‚   â”œâ”€â”€ 40-SimilaritySearch.md
    â”‚   â”œâ”€â”€ 50-Vgrep.md
    â”‚   â””â”€â”€ 60-Wrapup.md
    â”œâ”€â”€ cmd/
    â”‚   â””â”€â”€ readgguf/
    â”‚       â””â”€â”€ main.go
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ 1b.go
    â”‚   â””â”€â”€ Makefile
    â””â”€â”€ x/
        â””â”€â”€ quantization/
            â””â”€â”€ main.go

================================================
FILE: README.md
================================================
# What happens when you type a prompt into Ollama?

> Workshop at [GoLab](https://golab.io) 2025, 2025-10-05, [Martin Czygan](https://de.linkedin.com/in/martin-czygan-58348842)

## Overview

Ollama is a popular tool to run LLM (and multimodal and embedding models) on
your own machine. As of 09/2025 it has over 150K stars on GitHub.

There are numerous other tools to run models locally:

* [llama.cpp](https://github.com/ggml-org/llama.cpp)
* [llamafile](https://github.com/Mozilla-Ocho/llamafile)
* [vllm](https://github.com/vllm-project/vllm) (used by various cloud services)
* [OpenLLM](https://github.com/bentoml/OpenLLM)

And even more user interfaces of various kinds.

As of 09/2025, of the [25809 repositories](https://github.com/topics/llm) on GitHub
tagged [llm], ollama seems to be among the top ten.





================================================
FILE: go.mod
================================================
module github.com/miku/ollamaintro

go 1.25.1

require github.com/ollama/ollama v0.12.0

require (
	github.com/fatih/color v1.18.0 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	golang.org/x/sys v0.31.0 // indirect
)



================================================
FILE: go.sum
================================================
github.com/fatih/color v1.18.0 h1:S8gINlzdQ840/4pfAwic/ZE0djQEH3wM94VfqLTZcOM=
github.com/fatih/color v1.18.0/go.mod h1:4FelSpRwEGDpQ12mAdzqdOukCy4u8WUtOY6lkT/6HfU=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/mattn/go-colorable v0.1.13 h1:fFA4WZxdEF4tXPZVKMLwD8oUnCTTo08duU7wxecdEvA=
github.com/mattn/go-colorable v0.1.13/go.mod h1:7S9/ev0klgBDR4GtXTXX8a3vIGJpMovkB8vQcUbaXHg=
github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/ollama/ollama v0.12.0 h1:BRry7G2Skz7Mu+E6rz40tzBXNbLTEhheGT8umc1zvxo=
github.com/ollama/ollama v0.12.0/go.mod h1:9+1//yWPsDE2u+l1a5mpaKrYw4VdnSsRU3ioq5BvMms=
golang.org/x/sync v0.12.0 h1:MHc5BpPuC30uJk597Ri8TV3CNZcTLu6B6z4lJy+g6Jw=
golang.org/x/sync v0.12.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.31.0 h1:ioabZlmFYtWhL+TRYpcnNlLwhyxaM9kWTDEmfnprqik=
golang.org/x/sys v0.31.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Martin Czygan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: Refs.md
================================================
# References

* [https://tlbflush.org/post/2025_02_17_gguf_weekend/](https://tlbflush.org/post/2025_02_17_gguf_weekend/)



================================================
FILE: Slides.md
================================================
# Overview

* [Intro](10-Intro.md)
* [Background](20-Background.md)
* [Ollama architecture](30-Architecture.md)
    * [Server](32-Server.md)
    * [Runner](34-Runner.md)
    * [Tokenization](36-Tokenization.md)
* [Model lifecycle](40-Model-Lifecycle.md)
    * [Model Types](42-Model-Types.md)
    * [Model Parameters](44-Params.md)
    * [Templates](45-Templates.md)
    * [Model Format](46-GGUF.md)
    * [Modelfile customizations](48-Modelfile.md)
* [API](50-API.md)
* [Tracing a request](60-Request-Trace.md)
* [Ex: Similarity search](80-SimSearch.md)
* [Ex: Visual grep](90-VisualGrep.md)
* [Outro](99-Outro.md)



================================================
FILE: attic/10-Intro.md
================================================
# Welcome

* A short disclaimer, first!

## Overview

We have a few parts:

* Background, [Intro](Intro.md)
* Ollama internals
* Ollama customizations
* Usage **example**: Similarity search
* Usage **example**: Grep through handwritten notes
* Wrap-up

## Background

* interest in NLP since early 2000s,
  [wortschatz](https://wortschatz-leipzig.de/en)
([ex](https://dict.wortschatz-leipzig.de/en/res?corpusId=eng_news_2024&word=Uffizi+Gallery))
at Leipzig University

![Picture of a book: Dornseiff dictionary](static/9783110002874-de.jpg)

> 1933/34 unter dem Titel *Der deutsche Wortschatz synonymisch geordnet.* --
> [Dornseiff - Der deutsche Wortschatz nach
> Sachgruppen](https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/4961/file/Storjohann_Dornseiff_Der_deutsche_Wortschatz_nach_Sachgruppen_2012.pdf)

![](static/dornseiff-page.png)

Similar, popular, early project: [wordnet](https://en.wikipedia.org/wiki/WordNet):

> WordNet is a lexical database of semantic relations between words that links
> words into semantic relations including synonyms, hyponyms, and meronyms.

```sh
$ sudo apt install wordnet
$ wnb
```

![WordNet Browser Overview: go](static/screenshot-2025-09-20-170918-wordnet-go.png)

![WordNet Browser Hyponyms: train](static/screenshot-2025-09-20-170947-wordnet-train-hyponyms.png)

Statistical approach, counting words, sparse representation (bag-of-words), manual curation.

![](static/screenshot-2025-09-20-171719-gemini-norvig-quote.png)

Back then, there was an influencial paper about how more data wins over
algorithms, let me quickly ask an LLM which paper it was?

![](static/screenshot-2025-09-20-172118-claude-scaling-2001.png)

A few open weights LLMs struggle a bit with this question, but [Meta Llama 3.3
70B Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
(released December 6, 2024) seems to remember it.

![](static/screenshot-2025-09-20-172808-chatai-meta-llama3.3-70B-scaling-2001.png)

The paper was called: "Scaling to Very Very Large Corpora for Natural Language
Disambiguation" (2001)

> We collected a **1-billion-word** training corpus from a variety of English
> texts, including news articles, scientific abstracts, government transcripts,
> literature and other varied forms of prose. This training corpus is three
> orders of magnitude greater than the largest training corpus previously used
> for this problem.

You can generate 1B words in a few seconds today:

```
$ perf stat -r 1 -B ./1b > words.txt && du -h words.txt

 Performance counter stats for './1b':

          6,588.00 msec task-clock                       #    1.005 CPUs utilized
             4,216      context-switches                 #  639.951 /sec
                19      cpu-migrations                   #    2.884 /sec
               100      page-faults                      #   15.179 /sec
     6,480,220,375      cpu_atom/cycles/                 #    0.984 GHz                         (0.11%)
    24,934,230,875      cpu_core/cycles/                 #    3.785 GHz                         (99.45%)
     9,898,653,980      cpu_atom/instructions/           #    1.53  insn per cycle              (0.27%)
   122,415,155,922      cpu_core/instructions/           #    4.91  insn per cycle              (99.45%)
     2,911,671,826      cpu_atom/branches/               #  441.966 M/sec                       (0.35%)
    26,063,374,535      cpu_core/branches/               #    3.956 G/sec                       (99.45%)
        39,141,727      cpu_atom/branch-misses/          #    1.34% of all branches             (0.53%)
         4,678,785      cpu_core/branch-misses/          #    0.02% of all branches             (99.45%)
             TopdownL1 (cpu_core)                 #     10.6 %  tma_backend_bound
                                                  #      0.8 %  tma_bad_speculation
                                                  #     13.0 %  tma_frontend_bound
                                                  #     75.7 %  tma_retiring             (99.45%)
             TopdownL1 (cpu_atom)                 #     16.6 %  tma_bad_speculation
                                                  #     30.4 %  tma_retiring             (0.55%)
                                                  #     17.1 %  tma_backend_bound
                                                  #     35.9 %  tma_frontend_bound       (0.47%)

       6.556773774 seconds time elapsed

       5.171633000 seconds user
       1.444366000 seconds sys


4.7G    words.txt
```

## Some events and milestones

Cf. [arxiv sanity](https://github.com/karpathy/arxiv-sanity-preserver) (2016)

![](static/wayback-arxiv-sanity.png)

On the research side:

* 2003 "a neural probabilistic language model" (Y. Bengio et al.)
* 2013 word2vec (dense static vectors)
* 2014 fasttext (subword tokens)
* 2017 attention is all you need (no more recurrence, "transformer")
* 2018 BERT (comprehension) and original GPT-1 (generation)

...

> By the end of 2018, the field of NLP was about to undergo another
seismic change, marking the beginning of the era of foundation models -- [On
the Opportunities and Risks of Foundation
Models](https://arxiv.org/pdf/2108.07258)

* 2019 Language models are unsupervised multitask learners
* 2019 "bitter lesson"
* 2020 Language models are few shot learners
* 2020 "scaling laws"
* 2021 A General Language Assistant as a Laboratory for Alignment
* 2021 On the Opportunities and Risks of Foundation Models
* 2022 LLM are zero shot learners
* ...

List of open llms: [open-llms](https://github.com/eugeneyan/open-llms?tab=readme-ov-file#open-llms)

On the tooling/data side:

* 2015 tensorflow, keras
* 2016 pytorch
* 2018 jax, `pytorch_pretrained_bert` (later renamed HF transformers)
* 2020 "the pile" (800gb dataset)
* ...
* 2022-09-18 ggml (initial release)
* 2023-03-10 llama.cpp (initial release)
* 2023-07-20 ollama (initial release, support for two models)

And numerous more...

> Personal discoveries:

* 2023-02-14, I ask a question on how long before we can run things locally at the [Leipzig Python User Group](https://lpug.github.io/) -- personally, I expected 2-5 years timeline
* 2023-04-18, we discuss C/GO and ggml (ai-on-the-edge) at [Leipzig Gophers #35](https://golangleipzig.space/posts/meetup-35-wrapup/)
* 2023-07-20, [ollama](https://ollama.ai) is released (with two models), [HN](https://news.ycombinator.com/item?id=36802582)
* 2023-11-21, today, 43 models (each with a couple of tags/versions)




================================================
FILE: attic/20-Internals.md
================================================
# Internals

## Overview

Ollama covers a few aspects of the LLM inference life cycle:

* manage **model life cycle**: download, store, customize, publish, remove
* **interactive chat**
* expose various **API** (native, openai compat)

## Inspired by docker

* ollama runs as a server, service
* cli makes request to the local (or some remote server)

## Ollama registry

The ollama project maintains a registry, which is modelled after docker. With a
bit of work, you can run your own registry (cf.
[#2388](https://github.com/ollama/ollama/issues/2388#issuecomment-1989307410)).

However, you can download files from other servers, too.

## Ollama model location

* model files reside under a dot folder (depending on platform)

```
$ tree -d /usr/share/ollama/.ollama/
/usr/share/ollama/.ollama/
â””â”€â”€ models
    â”œâ”€â”€ blobs
    â””â”€â”€ manifests
        â”œâ”€â”€ hf.co
        â”‚Â Â  â”œâ”€â”€ arcee-ai
        â”‚Â Â  â”‚Â Â  â””â”€â”€ SuperNova-Medius-GGUF
        â”‚Â Â  â”œâ”€â”€ nomic-ai
        â”‚Â Â  â”‚Â Â  â””â”€â”€ nomic-embed-text-v2-moe-gguf
        â”‚Â Â  â”œâ”€â”€ TheBloke
        â”‚Â Â  â”‚Â Â  â””â”€â”€ Mistral-7B-Instruct-v0.1-GGUF
        â”‚Â Â  â”œâ”€â”€ unsloth
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Nanonets-OCR-s-GGUF
        â”‚Â Â  â”‚Â Â  â””â”€â”€ Qwen3-Coder-30B-A3B-Instruct-GGUF
        â”‚Â Â  â””â”€â”€ xtuner
        â”‚Â Â      â””â”€â”€ llava-llama-3-8b-v1_1-gguf
        â””â”€â”€ registry.ollama.ai
            â””â”€â”€ library
                â”œâ”€â”€ all-minilm
                â”œâ”€â”€ bge-large
                â”œâ”€â”€ gemma2
                â”œâ”€â”€ gemma3
                â”œâ”€â”€ gemma3n
                â”œâ”€â”€ gpt-oss
                â”œâ”€â”€ granite3.2
                â”œâ”€â”€ granite3.2-vision
                â”œâ”€â”€ granite-embedding
                â”œâ”€â”€ llama3
                â”œâ”€â”€ llama3.2
                â”œâ”€â”€ llama3.2-vision
                â”œâ”€â”€ minicpm-v
                â”œâ”€â”€ mistral
                â”œâ”€â”€ mistral-nemo
                â”œâ”€â”€ mistral-small
                â”œâ”€â”€ mistral-small3.1
                â”œâ”€â”€ mistral-small3.2
                â”œâ”€â”€ moondream
                â”œâ”€â”€ mxbai-embed-large
                â”œâ”€â”€ nomic-embed-text
                â”œâ”€â”€ nomic-embed-text-v2
                â”œâ”€â”€ paraphrase-multilingual
                â”œâ”€â”€ qwen2.5-coder
                â”œâ”€â”€ qwen2.5vl
                â”œâ”€â”€ qwen3
                â”œâ”€â”€ qwen3-coder
                â”œâ”€â”€ smollm
                â””â”€â”€ smollm2

47 directories
```

We have manifests and blobs. Manifests link to blobs. You can download from
various model providers and also run your own. Default "registry.ollama.ai"
library contains official models (and variants people push/host there).

```
$ tree /usr/share/ollama/.ollama/models/manifests/ | head -40
/usr/share/ollama/.ollama/models/manifests/
â”œâ”€â”€ hf.co
â”‚Â Â  â”œâ”€â”€ arcee-ai
â”‚Â Â  â”‚Â Â  â””â”€â”€ SuperNova-Medius-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ Q8_0
â”‚Â Â  â”œâ”€â”€ nomic-ai
â”‚Â Â  â”‚Â Â  â””â”€â”€ nomic-embed-text-v2-moe-gguf
â”‚Â Â  â”‚Â Â      â””â”€â”€ latest
â”‚Â Â  â”œâ”€â”€ TheBloke
â”‚Â Â  â”‚Â Â  â””â”€â”€ Mistral-7B-Instruct-v0.1-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ latest
â”‚Â Â  â”œâ”€â”€ unsloth
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Nanonets-OCR-s-GGUF
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ BF16
â”‚Â Â  â”‚Â Â  â””â”€â”€ Qwen3-Coder-30B-A3B-Instruct-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ UD-Q4_K_XL
â”‚Â Â  â””â”€â”€ xtuner
â”‚Â Â      â””â”€â”€ llava-llama-3-8b-v1_1-gguf
â”‚Â Â          â””â”€â”€ latest
â””â”€â”€ registry.ollama.ai
    â””â”€â”€ library
        â”œâ”€â”€ all-minilm
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ bge-large
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gemma2
        â”‚Â Â  â””â”€â”€ 2b
        â”œâ”€â”€ gemma3
        â”‚Â Â  â”œâ”€â”€ 12b
        â”‚Â Â  â”œâ”€â”€ 1b
        â”‚Â Â  â”œâ”€â”€ 270m
        â”‚Â Â  â”œâ”€â”€ 27b
        â”‚Â Â  â”œâ”€â”€ 27b-it-qat
        â”‚Â Â  â”œâ”€â”€ 4b
        â”‚Â Â  â”œâ”€â”€ 4b-it-qat
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gemma3n
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gpt-oss
        â”‚Â Â  â”œâ”€â”€ 20b
```

## Manifest file

A manifest groups the files that make up a model, including the raw weights, templates, license.

```json
$ cat /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library/gemma3n/latest | jq .
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "digest": "sha256:8eac5d7750c5bd24a9c556890ecb08ff749fefb7c4952b8962c5e7835aef21be",
    "size": 491
  },
  "layers": [
    {
      "mediaType": "application/vnd.ollama.image.model",
      "digest": "sha256:38e8dcc30df4eb0e29eaf5c74ba6ce3f2cd66badad50768fc14362acfb8b8cb6",
      "size": 7547579904
    },
    {
      "mediaType": "application/vnd.ollama.image.template",
      "digest": "sha256:e0a42594d802e5d31cdc786deb4823edb8adff66094d49de8fffe976d753e348",
      "size": 358
    },
    {
      "mediaType": "application/vnd.ollama.image.license",
      "digest": "sha256:1adbfec9dcf025cbf301c072f3847527468dcfa399da7491ee4a1c9e9f1b33e9",
      "size": 8363
    }
  ]
}
```

A sampling of the media types:

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[].mediaType' | sort | uniq -c | sort -nr

     45 application/vnd.ollama.image.model
     41 application/vnd.ollama.image.license
     36 application/vnd.ollama.image.params
     35 application/vnd.ollama.image.template
      8 application/vnd.ollama.image.system
      4 application/vnd.ollama.image.projector
```

### MediaType: model

The actual model, here in GGUF format. It includes metadata and the network layers.

See: [22-GGUF.md](22-GGUF.md)

### MediaType: licence

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[] | select(.mediaType == "application/vnd.ollama.image.license") | .digest'  | shuf -n 10

sha256:832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e
sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:dd084c7d92a3c1c14cc09ae77153b903fd2024b64a100a0cc8ec9316063d2dbc
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:cfc7749b96f63bd31c3c42b5c471bf756814053e847c10f3eb003417bc523d30
sha256:3e2c24001f9ef57bf7ec959a3658fbb49cdad113cdf394c264da9d16f9bdd132
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:a406579cd136771c705c521db86ca7d60a6f3de7c9b5460e6193a2df27861bde
sha256:2e68075caee4c571d43cb9d8d636415d8fe0fe59f695370cf56bc9f872f1ff3f
```

Example license:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e | head

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
```


### MediaType: params

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[] | select(.mediaType == "application/vnd.ollama.image.params") | .digest'  | shuf -n 10

sha256:832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e
sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:dd084c7d92a3c1c14cc09ae77153b903fd2024b64a100a0cc8ec9316063d2dbc
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:cfc7749b96f63bd31c3c42b5c471bf756814053e847c10f3eb003417bc523d30
sha256:3e2c24001f9ef57bf7ec959a3658fbb49cdad113cdf394c264da9d16f9bdd132
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:a406579cd136771c705c521db86ca7d60a6f3de7c9b5460e6193a2df27861bde
sha256:2e68075caee4c571d43cb9d8d636415d8fe0fe59f695370cf56bc9f872f1ff3f
```

Example params:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-339e884a40f6708bc761d367f0c08e448d5bb6f16b3961c340e44e0e4835a004 | jq .
{
  "stop": [
    "<end_of_turn>"
  ],
  "top_k": 64,
  "top_p": 0.95
}

$ cat /usr/share/ollama/.ollama/models/blobs/sha256-e0daf17ff83eace4813f9e8554b262f6cc33ad880ff8df41a156ff9ef5522ddb | jq .
{
  "temperature": 0.15
}
```

Typical parameters:

```
$ ollama ls  | grep -v NAME | awk '{print $1}' | \
    xargs -I {} ollama show --parameters {} | sort  | awk '{print $1}' | uniq -c | sort -nr
     44 stop
     24 temperature
     15 top_p
     12 top_k
     12
      7 num_ctx
      5 repeat_penalty
      2 min_p
      1 num_keep
```


### MediaType: system

A system prompt.

```shell
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33

You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI,
a French startup headquartered in Paris.  You power an AI assistant called Le
Chat.  Your knowledge base was last updated on 2023-10-01.

When you're not sure about some information, you say that you don't have the
information and don't make up anything.  If the user's question is not clear,
ambiguous, or does not provide enough context for you to accurately answer the
question, you do not try to answer it right away and you rather ask the user to
clarify their request (e.g. "What are some good restaurants around me?" =>
"Where are you?" or "When is the next flight to Tokyo" => "Where do you travel
from?").  You are always very attentive to dates, in particular you try to
resolve dates (e.g. "yesterday" is {yesterday}) and when asked about
information at specific dates, you discard information that is at another date.
You follow these instructions in all languages, and always respond to the user
in the language they use or request.  Next sections describe the capabilities
that you have.

# WEB BROWSING INSTRUCTIONS

You cannot perform any web search or access internet to open URLs, links etc.
If it seems like the user is expecting you to do so, you clarify the situation
and ask the user to copy paste the text directly in the chat.

# MULTI-MODAL INSTRUCTIONS

You have the ability to read images, but you cannot generate images. You also
cannot transcribe audio files or videos.
```

Or just:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-75357d685f238b6afd7738be9786fdafde641eb6ca9a3be7471939715a68a4de
You are a helpful assistant.
```

### MediaType: projector

GGUF file, component for vision models.

```
ğŸ“‹ Model Metadata
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

General:
  general.description â”‚ image encoder for LLaVA
  general.name        â”‚ siglip-model
...
```


## Getting quick model info

Display model properties and parameters with `ollama show` subcommand:

```
$ ollama show gemma3:4b-it-qat
  Model
    architecture        gemma3
    parameters          4.3B
    context length      131072
    embedding length    2560
    quantization        Q4_0

  Capabilities
    completion
    vision

  Parameters
    top_p          0.95
    stop           "<end_of_turn>"
    temperature    1
    top_k          64

```

Extended help:






## Ollama Codebase

About 78K LOC Go. Ollama vendors [ggml](https://github.com/ggml-org/ggml) and
[llama.cpp](https://github.com/ggml-org/llama.cpp), which are C and C++ projects.

```
$ tokei
===============================================================================
 Language            Files        Lines         Code     Comments       Blanks
===============================================================================
 GNU Style Assembly      1            6            6            0            0
 Autoconf                1            4            4            0            0
 C                      12        26550        21425          947         4178
 C Header               87       122062       103618        11551         6893
 CMake                   8         1739         1403          112          224
 C++                    58        95684        75391         5559        14734
 C++ Header              3        26105        18285         4684         3136
 CSS                     1           34           29            0            5
 Dockerfile              1          123          104            3           16
 Go                    327        78674        64719         4230         9725
 HTML                    1            9            9            0            0
 JavaScript              2           13           12            1            0
 JSON                   38       147592       147592            0            0
 Objective-C             2         6816         5624          233          959
 PowerShell              2          265          242            9           14
 Protocol Buffers        1          333           98          178           57
 Shell                   7          608          476           39           93
 SVG                     1            9            9            0            0
 Plain Text              4       190212            0       167280        22932
 TSX                     2          129          122            0            7
 TypeScript              9          494          407           14           73
-------------------------------------------------------------------------------
 Markdown               25         3659            0         2439         1220
 |- BASH                 1            1            1            0            0
 |- Dockerfile           3           22           21            0            1
 |- Go                   1           23           22            0            1
 |- INI                  2           18           16            0            2
 |- JavaScript           1           43           34            1            8
 |- JSON                 1          504          504            0            0
 |- PowerShell           3            5            5            0            0
 |- Python               2           96           78            2           16
 |- Shell               15          629          624            0            5
 |- TypeScript           1           18           15            0            3
 (Total)                           5018         1320         2442         1256
===============================================================================
 Total                 593       701120       439575       197279        64266
===============================================================================
```


## High Level Request Flow

1. **Request Reception**: Client sends HTTP request to Ollama server (port
   11434) via CLI, REST API, or OpenAI-compatible endpoint

2. **Request Routing**: Server routes request through `server/routes.go` to
   appropriate handler (`/api/generate`, `/api/chat`, etc.)

3. **Model Management**: Scheduler (`server/sched.go`) checks if model is
   loaded; if not, loads model into memory with appropriate backend (CUDA/ROCm/Metal)

4. **Prompt Processing**: Request is parsed and prepared for inference by the
   runner system

5. **Inference Execution**: Specialized runner (`runner/`) performs model
   inference using optimized backends for token generation

6. **Response Streaming**: Generated tokens are streamed back to client via
   HTTP streaming (default mode) or returned as complete response

7. **Resource Management**: System manages GPU/CPU resources and model
   lifecycle for subsequent requests


## Model files



================================================
FILE: attic/21-Arch.md
================================================
# Ollama architecture

## Core ideas

* local execution (of models in GGUF format)
* model lifecycle (registry, customizations)
* various interfaces: chat, API

It is possible to get gguf files from any remote

## Client/Server

* server running as a service, exposing various API
* cli as API client
* easy to run or share a remote instance


```go
    // General
    r.HEAD("/", func(c *gin.Context) { c.String(http.StatusOK, "Ollama is running") })
    r.GET("/", func(c *gin.Context) { c.String(http.StatusOK, "Ollama is running") })
    r.HEAD("/api/version", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{"version": version.Version}) })
    r.GET("/api/version", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{"version": version.Version}) })

    // Local model cache management (new implementation is at end of function)
    r.POST("/api/pull", s.PullHandler)
    r.POST("/api/push", s.PushHandler)
    r.HEAD("/api/tags", s.ListHandler)
    r.GET("/api/tags", s.ListHandler)
    r.POST("/api/show", s.ShowHandler)
    r.DELETE("/api/delete", s.DeleteHandler)

    // Create
    r.POST("/api/create", s.CreateHandler)
    r.POST("/api/blobs/:digest", s.CreateBlobHandler)
    r.HEAD("/api/blobs/:digest", s.HeadBlobHandler)
    r.POST("/api/copy", s.CopyHandler)

    // Inference
    r.GET("/api/ps", s.PsHandler)
    r.POST("/api/generate", s.GenerateHandler)
    r.POST("/api/chat", s.ChatHandler)
    r.POST("/api/embed", s.EmbedHandler)
    r.POST("/api/embeddings", s.EmbeddingsHandler)

    // Inference (OpenAI compatibility)
    r.POST("/v1/chat/completions", openai.ChatMiddleware(), s.ChatHandler)
    r.POST("/v1/completions", openai.CompletionsMiddleware(), s.GenerateHandler)
    r.POST("/v1/embeddings", openai.EmbeddingsMiddleware(), s.EmbedHandler)
    r.GET("/v1/models", openai.ListMiddleware(), s.ListHandler)
    r.GET("/v1/models/:model", openai.RetrieveMiddleware(), s.ShowHandler)
```


## Path of a generate request

> Generate a response for a given prompt with a provided model. -- [docs](https://ollama.readthedocs.io/en/api/#generate-a-completion)


```go
// GenerateRequest describes a request sent by [Client.Generate]. While you
// have to specify the Model and Prompt fields, all the other fields have
// reasonable defaults for basic uses.
type GenerateRequest struct {
    // Model is the model name; it should be a name familiar to Ollama from
    // the library at https://ollama.com/library
    Model string `json:"model"`

    // Prompt is the textual prompt to send to the model.
    Prompt string `json:"prompt"`

    // Suffix is the text that comes after the inserted text.
    Suffix string `json:"suffix"`

    // System overrides the model's default system message/prompt.
    System string `json:"system"`

    // Template overrides the model's default prompt template.
    Template string `json:"template"`

    // Context is the context parameter returned from a previous call to
    // [Client.Generate]. It can be used to keep a short conversational memory.
    Context []int `json:"context,omitempty"`

    // Stream specifies whether the response is streaming; it is true by default.
    Stream *bool `json:"stream,omitempty"`

    // Raw set to true means that no formatting will be applied to the prompt.
    Raw bool `json:"raw,omitempty"`

    // Format specifies the format to return a response in.
    Format json.RawMessage `json:"format,omitempty"`

    // KeepAlive controls how long the model will stay loaded in memory following
    // this request.
    KeepAlive *Duration `json:"keep_alive,omitempty"`

    // Images is an optional list of raw image bytes accompanying this
    // request, for multimodal models.
    Images []ImageData `json:"images,omitempty"`

    // Options lists model-specific options. For example, temperature can be
    // set through this field, if the model supports it.
    Options map[string]any `json:"options"`

    // Think controls whether thinking/reasoning models will think before
    // responding. Can be a boolean (true/false) or a string ("high", "medium", "low")
    // for supported models. Needs to be a pointer so we can distinguish between false
    // (request that thinking _not_ be used) and unset (use the old behavior
    // before this option was introduced)
    Think *ThinkValue `json:"think,omitempty"`

    // DebugRenderOnly is a debug option that, when set to true, returns the rendered
    // template instead of calling the model.
    DebugRenderOnly bool `json:"_debug_render_only,omitempty"`
}
```

The completion model parses a model struct, hovering over the manifest.

```go
func GetModel(name string) (*Model, error) {
    mp := ParseModelPath(name)
    manifest, digest, err := GetManifest(mp)
    if err != nil {
        return nil, err
    }

    model := &Model{
        Name:      mp.GetFullTagname(),
        ShortName: mp.GetShortTagname(),
        Digest:    digest,
        Template:  template.DefaultTemplate,
    }

    if manifest.Config.Digest != "" {
        filename, err := GetBlobsPath(manifest.Config.Digest)
        if err != nil {
            return nil, err
        }

        configFile, err := os.Open(filename)
        if err != nil {
            return nil, err
        }
        defer configFile.Close()

        if err := json.NewDecoder(configFile).Decode(&model.Config); err != nil {
            return nil, err
        }
    }

    for _, layer := range manifest.Layers {
        filename, err := GetBlobsPath(layer.Digest)
        if err != nil {
            return nil, err
        }

        switch layer.MediaType {
        case "application/vnd.ollama.image.model":
            model.ModelPath = filename
            model.ParentModel = layer.From
        case "application/vnd.ollama.image.embed":
            // Deprecated in versions  > 0.1.2
            // TODO: remove this warning in a future version
            slog.Info("WARNING: model contains embeddings, but embeddings in modelfiles have been deprecated and will be ignored.")
        case "application/vnd.ollama.image.adapter":
            model.AdapterPaths = append(model.AdapterPaths, filename)
        case "application/vnd.ollama.image.projector":
            model.ProjectorPaths = append(model.ProjectorPaths, filename)
        case "application/vnd.ollama.image.prompt",
            "application/vnd.ollama.image.template":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }

            model.Template, err = template.Parse(string(bts))
            if err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.system":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }

            model.System = string(bts)
        case "application/vnd.ollama.image.params":
            params, err := os.Open(filename)
            if err != nil {
                return nil, err
            }
            defer params.Close()

            // parse model options parameters into a map so that we can see which fields have been specified explicitly
            if err = json.NewDecoder(params).Decode(&model.Options); err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.messages":
            msgs, err := os.Open(filename)
            if err != nil {
                return nil, err
            }
            defer msgs.Close()

            if err = json.NewDecoder(msgs).Decode(&model.Messages); err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.license":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }
            model.License = append(model.License, string(bts))
        }
    }

    return model, nil
}
```

This is more like model metadata:

```

type Model struct {
    Name           string `json:"name"`
    Config         ConfigV2
    ShortName      string
    ModelPath      string
    ParentModel    string
    AdapterPaths   []string
    ProjectorPaths []string
    System         string
    License        []string
    Digest         string
    Options        map[string]any
    Messages       []api.Message

    Template *template.Template
}
```

A first look into the request prompt, if it is empty we unload the model.

Some models have custom reponse renderers, like gptoss
[harmony](https://cookbook.openai.com/articles/openai-harmony).

Once the request input is validated, as runner is requested:

```
r, m, opts, err := s.scheduleRunner(c.Request.Context(), name.String(), caps, req.Options, req.KeepAlive)
```

Schedule runner will validated model capabilities, and will call `GetRunner`

```
    runnerCh, errCh := s.sched.GetRunner(ctx, model, opts, keepAlive)
```

The scheduler determines if the request could be satisfied by a loaded runner,
otherwise it is send to a pending requests queue.

When the model is available, we proceed to add images and preparing the prompt.

```go
  images := make([]llm.ImageData, len(req.Images))
    for i := range req.Images {
        images[i] = llm.ImageData{ID: i, Data: req.Images[i]}
    }
```

The server ask LlamaServer for completion.

```
        if err := r.Completion(c.Request.Context(), llm.CompletionRequest{
            Prompt:     prompt,
            Images:     images,
            Format:     req.Format,
            Options:    opts,
            UseHarmony: useHarmony,
        }, func(cr llm.CompletionResponse) {
            res := api.GenerateResponse{
                Model:     req.Model,
                CreatedAt: time.Now().UTC(),
                Response:  cr.Content,
                Done:      cr.Done,
                Thinking:  cr.Thinking,
                ToolCalls: cr.ToolCalls,
                Metrics: api.Metrics{
                    PromptEvalCount:    cr.PromptEvalCount,
                    PromptEvalDuration: cr.PromptEvalDuration,
                    EvalCount:          cr.EvalCount,
                    EvalDuration:       cr.EvalDuration,
                },
            }
```


## The runner process

Ollama has two runner options.

```
    if newRunner {
        return ollamarunner.Execute(args)
    } else {
        return llamarunner.Execute(args)
    }
```

## Input

```
// Input represents one token in the input stream
type Input struct {
    // Token is a single element of text.
    Token int32

    // Multimodal is represents a non-text element such as an
    // image (or part of one if the image can be processed in pieces).
    // It may be used either together with Token or on its own.
    Multimodal []Multimodal

    // MultimodalHash is a unique representation of the data
    // stored in Multimodal, used for caching and comparing
    // equality.
    MultimodalHash uint64

    // SameBatch forces the following number of tokens to be processed
    // in a single batch, breaking and extending batches as needed.
    // Useful for things like images that must be processed in one
    // shot.
    SameBatch int
}
```

## Batch

```
// Batch contains the inputs for a model forward pass
type Batch struct {
    // Inputs is the input tokens, including placeholders for multimodal inputs.
    Inputs ml.Tensor

    // Outputs are the set of indicies into Inputs for which output data should
    // be returned.
    Outputs ml.Tensor

    // Positions is the position for each Input, relative to its sequence. Equal
    // in length to Inputs.
    Positions []int32

    // Sequences is the sequence for each Input. Equal in length to Inputs.
    Sequences []int

    // Multimodal is a set of multimodal embeddings previously created by
    // EncodeMultimodal, along with an index into Inputs. Unused for text-only
    // models or for batches without multimodal elements.
    Multimodal []MultimodalIndex
}
```


## Loading and serving models

* models need to be loaded into memory
* diverse backends: CPU, GPU
* manage multiple models and requests (swapping models if required)





================================================
FILE: attic/22-GGUF.md
================================================
# GGUF

## Overview

See also: [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

> GGUF is a file format for storing **models for inference with GGML** and
> executors based on GGML. GGUF is a binary format that is designed for fast
> loading and saving of models, and for ease of reading. Models are
> traditionally developed using PyTorch or another framework, and then
> converted to GGUF for use in GGML.

Some format evolution:

> GGML, GGMF, GGJT, and now GGUF â€” and it is now at **version three** of the format.

Full description: [gguf.md](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

## Format overview

* magic
* version
* number of tensors
* number of key value pairs (metadata)

![](static/313174776-c3623641-3a1d-408e-bfaf-1b7c4e16aa63.png)

## Naming convention

```
<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf
```

Examples:

```
Mixtral-8x7B-v0.1-KQ2.gguf
Hermes-2-Pro-Llama-3-8B-F16.gguf
Grok-100B-v1.0-Q4_0-00003-of-00009.gguf
```

## readgguf

Example cli application: readgguf. You can point it to a file or URL to get
some information about gguf file.

Random, example project:
[SmolDocling-256M-preview-GGUF](https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF),
[files and
versions](https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF/tree/main)

```
$ curl -sLo SmolDocling-256M-preview-q8_0.gguf "https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF/resolve/main/SmolDocling-256M-preview-q8_0.gguf?download=true"
```

## Quantization

* [Blind testing different quants](https://github.com/ggml-org/llama.cpp/discussions/5962)



================================================
FILE: attic/25-Eval.md
================================================
# Evaluation

Leaderboards, e.g.

* [https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro)

Example test:

* [Ollama-MMLU](https://github.com/chigkim/Ollama-MMLU-Pro/)

Adapted from [MMLU-Pro](https://github.com/TIGER-AI-Lab/MMLU-Pro)

> We introduce MMLU-Pro, an enhanced benchmark designed to evaluate language
> understanding models across broader and more challenging tasks. Building on
> the Massive Multitask Language Understanding (MMLU) dataset, MMLU-Pro
> integrates more challenging, reasoning-focused questions and increases the
> answer choices per question from four to ten, significantly raising the
> difficulty and reducing the chance of success through random guessing.
> MMLU-Pro comprises over 12,000 rigorously curated questions from academic
> exams and textbooks, spanning 14 diverse domains including Biology, Business,
> Chemistry, Computer Science, Economics, Engineering, Health, History, Law,
> Math, Philosophy, Physics, Psychology, and Others.

## Issues

* isolated tests
* training on test cases (for a random note, see various comments on r/locallama, e.g. [n6hg2rc](https://www.reddit.com/r/LocalLLaMA/comments/1mfitwb/comment/n6hg2rc/))



================================================
FILE: attic/25-Tokenization.md
================================================
[Empty file]


================================================
FILE: attic/30-Customizations.md
================================================
[Empty file]


================================================
FILE: attic/40-SimilaritySearch.md
================================================
[Empty file]


================================================
FILE: attic/50-Vgrep.md
================================================
[Empty file]


================================================
FILE: attic/60-Wrapup.md
================================================
[Empty file]


================================================
FILE: cmd/readgguf/main.go
================================================
[Binary file]


================================================
FILE: scripts/1b.go
================================================
package main

import (
	"bufio"
	"io"
	"os"
)

func main() {
	bw := bufio.NewWriter(os.Stdout)
	defer bw.Flush()
	for _ = range 1_000_000_000 {
		_, _ = io.WriteString(bw, "word\n")
	}
}



================================================
FILE: scripts/Makefile
================================================
SHELL = /bin/bash

1b: 1b.go
	go build -o $@ $<

.PHONY: clean
clean:
	rm -f 1b



================================================
FILE: x/quantization/main.go
================================================
package main

import (
	"fmt"
	"math"
)

// QuantizationParams holds the parameters needed for quantization/dequantization
type QuantizationParams struct {
	Scale     float32 // Scale factor for mapping float range to int range
	ZeroPoint int8    // Zero point offset
	MinVal    float32 // Original minimum value
	MaxVal    float32 // Original maximum value
}

// Matrix represents a simple 2D matrix
type Matrix struct {
	Data [][]float32
	Rows int
	Cols int
}

// QuantizedMatrix represents a quantized version using int8
type QuantizedMatrix struct {
	Data   [][]int8
	Rows   int
	Cols   int
	Params QuantizationParams
}

// NewMatrix creates a new matrix with the given dimensions
func NewMatrix(rows, cols int) *Matrix {
	data := make([][]float32, rows)
	for i := range data {
		data[i] = make([]float32, cols)
	}
	return &Matrix{Data: data, Rows: rows, Cols: cols}
}

// calculateQuantizationParams determines the scale and zero point for quantization
func calculateQuantizationParams(minVal, maxVal float32) QuantizationParams {
	// For int8: range is -128 to 127 (256 possible values)
	const qMin, qMax int8 = -128, 127
	const qRange = float32(256) // 256 possible values in int8

	// Calculate scale: how much each quantized unit represents in float space
	scale := (maxVal - minVal) / qRange

	// Calculate zero point: where 0.0 maps to in quantized space
	zeroPoint := int8(math.Round(float64(float32(qMin) - minVal/scale)))

	// Clamp zero point to valid range
	if zeroPoint < qMin {
		zeroPoint = qMin
	}
	if zeroPoint > qMax {
		zeroPoint = qMax
	}

	return QuantizationParams{
		Scale:     scale,
		ZeroPoint: zeroPoint,
		MinVal:    minVal,
		MaxVal:    maxVal,
	}
}

// findMinMax finds the minimum and maximum values in the matrix
func (m *Matrix) findMinMax() (float32, float32) {
	if m.Rows == 0 || m.Cols == 0 {
		return 0, 0
	}

	minVal := m.Data[0][0]
	maxVal := m.Data[0][0]

	for i := 0; i < m.Rows; i++ {
		for j := 0; j < m.Cols; j++ {
			if m.Data[i][j] < minVal {
				minVal = m.Data[i][j]
			}
			if m.Data[i][j] > maxVal {
				maxVal = m.Data[i][j]
			}
		}
	}

	return minVal, maxVal
}

// Quantize converts the float32 matrix to int8 using calculated parameters
func (m *Matrix) Quantize() *QuantizedMatrix {
	minVal, maxVal := m.findMinMax()
	params := calculateQuantizationParams(minVal, maxVal)

	// Create quantized matrix
	qData := make([][]int8, m.Rows)
	for i := range qData {
		qData[i] = make([]int8, m.Cols)
	}

	// Quantization formula: q = round((x - zero_point_float) / scale) + zero_point_int
	// Simplified: q = round(x / scale) + zero_point
	for i := 0; i < m.Rows; i++ {
		for j := 0; j < m.Cols; j++ {
			// Map float value to quantized range
			quantized := math.Round(float64(m.Data[i][j]/params.Scale)) + float64(params.ZeroPoint)

			// Clamp to int8 range
			if quantized < -128 {
				quantized = -128
			}
			if quantized > 127 {
				quantized = 127
			}

			qData[i][j] = int8(quantized)
		}
	}

	return &QuantizedMatrix{
		Data:   qData,
		Rows:   m.Rows,
		Cols:   m.Cols,
		Params: params,
	}
}

// Dequantize converts the quantized matrix back to float32
func (qm *QuantizedMatrix) Dequantize() *Matrix {
	data := make([][]float32, qm.Rows)
	for i := range data {
		data[i] = make([]float32, qm.Cols)
	}

	// Dequantization formula: x = scale * (q - zero_point)
	for i := 0; i < qm.Rows; i++ {
		for j := 0; j < qm.Cols; j++ {
			dequantized := qm.Params.Scale * float32(qm.Data[i][j]-qm.Params.ZeroPoint)
			data[i][j] = dequantized
		}
	}

	return &Matrix{Data: data, Rows: qm.Rows, Cols: qm.Cols}
}

// Print displays the matrix values
func (m *Matrix) Print(name string) {
	fmt.Printf("\n%s (%dx%d):\n", name, m.Rows, m.Cols)
	for i := 0; i < m.Rows; i++ {
		for j := 0; j < m.Cols; j++ {
			fmt.Printf("%8.4f ", m.Data[i][j])
		}
		fmt.Println()
	}
}

// Print displays the quantized matrix values
func (qm *QuantizedMatrix) Print(name string) {
	fmt.Printf("\n%s (%dx%d) [int8]:\n", name, qm.Rows, qm.Cols)
	for i := 0; i < qm.Rows; i++ {
		for j := 0; j < qm.Cols; j++ {
			fmt.Printf("%4d ", qm.Data[i][j])
		}
		fmt.Println()
	}
	fmt.Printf("Scale: %.6f, ZeroPoint: %d, Range: [%.4f, %.4f]\n",
		qm.Params.Scale, qm.Params.ZeroPoint, qm.Params.MinVal, qm.Params.MaxVal)
}

// calculateMemoryUsage returns memory usage in bytes for comparison
func (m *Matrix) calculateMemoryUsage() int {
	return m.Rows * m.Cols * 4 // 4 bytes per float32
}

func (qm *QuantizedMatrix) calculateMemoryUsage() int {
	return qm.Rows*qm.Cols*1 + 4 + 1 + 4 + 4 // 1 byte per int8 + params
}

func main() {
	fmt.Println("=== LLM Quantization Demonstration ===")
	fmt.Println("This demonstrates the core concept of quantization used in efficient LLM inference")

	// Create a sample weight matrix (like a small part of an LLM layer)
	rows, cols := 4, 5
	original := NewMatrix(rows, cols)

	// Fill with some typical weight values that might appear in an LLM
	weights := [][]float32{
		{0.1234, -0.5678, 0.9012, -0.3456, 0.7890},
		{-0.2345, 0.6789, -0.1012, 0.4567, -0.8901},
		{0.3456, -0.7890, 0.2123, -0.5678, 0.9012},
		{-0.4567, 0.8901, -0.3234, 0.6789, -0.1012},
	}

	for i := 0; i < rows; i++ {
		for j := 0; j < cols; j++ {
			original.Data[i][j] = weights[i][j]
		}
	}

	// Show original matrix
	original.Print("Original FP32 Matrix")

	// Quantize to int8
	quantized := original.Quantize()
	quantized.Print("Quantized INT8 Matrix")

	// Dequantize back to float32
	dequantized := quantized.Dequantize()
	dequantized.Print("Dequantized FP32 Matrix")

	// Calculate and show memory savings
	originalMemory := original.calculateMemoryUsage()
	quantizedMemory := quantized.calculateMemoryUsage()
	savings := float64(originalMemory-quantizedMemory) / float64(originalMemory) * 100

	fmt.Printf("\n=== Memory Usage Comparison ===\n")
	fmt.Printf("Original FP32: %d bytes\n", originalMemory)
	fmt.Printf("Quantized INT8: %d bytes (including params)\n", quantizedMemory)
	fmt.Printf("Memory savings: %.1f%%\n", savings)

	// Calculate quantization error
	fmt.Printf("\n=== Quantization Error Analysis ===\n")
	var totalError float64
	var maxError float64

	for i := 0; i < rows; i++ {
		for j := 0; j < cols; j++ {
			error := math.Abs(float64(original.Data[i][j] - dequantized.Data[i][j]))
			totalError += error
			if error > maxError {
				maxError = error
			}
		}
	}

	avgError := totalError / float64(rows*cols)
	fmt.Printf("Average quantization error: %.6f\n", avgError)
	fmt.Printf("Maximum quantization error: %.6f\n", maxError)

	fmt.Println("\n=== How This Applies to Real LLM Frameworks ===")
	fmt.Println(`
In production LLM quantization frameworks:

1. **Layer-wise Quantization**: Each layer (attention, MLP) may have different
   quantization parameters optimized for its value distribution.

2. **Weight vs Activation Quantization**:
   - Weights: Quantized once during model conversion (static)
   - Activations: Quantized dynamically during inference

3. **Mixed Precision**: Critical layers might stay in FP16/FP32 while others
   use INT8/INT4 to balance accuracy and speed.

4. **Computation Graph Adaptation**:
   - Some ops work directly on quantized values (element-wise add)
   - Matrix multiplication often requires dequantization
   - Special kernels can perform quantized matrix operations
   - Batch norm and layer norm may need special handling

5. **Framework Integration**:
   - PyTorch: torch.quantization with QConfig
   - TensorRT: Automatic quantization-aware training
   - ONNX Runtime: Post-training quantization

The key insight: Quantization trades precision for memory/speed, requiring
careful calibration to maintain model quality while achieving efficiency gains.`)
}


