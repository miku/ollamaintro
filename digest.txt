Directory structure:
â””â”€â”€ ollamaintro/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ 10-Intro.md
    â”œâ”€â”€ 20-Background.md
    â”œâ”€â”€ 30-Architecture.md
    â”œâ”€â”€ 32-Server.md
    â”œâ”€â”€ 34-Runner.md
    â”œâ”€â”€ 36-Tokenization.md
    â”œâ”€â”€ 40-Model-Lifecycle.md
    â”œâ”€â”€ 42-Model-Types.md
    â”œâ”€â”€ 44-Params.md
    â”œâ”€â”€ 45-Templates.md
    â”œâ”€â”€ 46-GGUF.md
    â”œâ”€â”€ 48-Modelfile.md
    â”œâ”€â”€ 50-API.md
    â”œâ”€â”€ 60-Request-Trace.md
    â”œâ”€â”€ 80-SimSearch.md
    â”œâ”€â”€ 90-VisualGrep.md
    â”œâ”€â”€ 99-Outro.md
    â”œâ”€â”€ Refs.md
    â”œâ”€â”€ Slides.md
    â”œâ”€â”€ attic/
    â”‚   â”œâ”€â”€ 10-Intro.md
    â”‚   â”œâ”€â”€ 20-Internals.md
    â”‚   â”œâ”€â”€ 21-Arch.md
    â”‚   â”œâ”€â”€ 22-GGUF.md
    â”‚   â”œâ”€â”€ 25-Eval.md
    â”‚   â”œâ”€â”€ 25-Tokenization.md
    â”‚   â”œâ”€â”€ 30-Customizations.md
    â”‚   â”œâ”€â”€ 40-SimilaritySearch.md
    â”‚   â”œâ”€â”€ 50-Vgrep.md
    â”‚   â””â”€â”€ 60-Wrapup.md
    â”œâ”€â”€ extra/
    â”‚   â”œâ”€â”€ ModelFormats.md
    â”‚   â””â”€â”€ Quantization.md
    â””â”€â”€ x/
        â””â”€â”€ seq/
            â””â”€â”€ README.md

================================================
FILE: README.md
================================================
# What happens when you type a prompt into Ollama?

> Workshop at [GoLab](https://golab.io) 2025, 2025-10-05, [Martin Czygan](https://de.linkedin.com/in/martin-czygan-58348842)

## Overview

Ollama is a popular tool to run LLM (and multimodal and embedding models) on
your own machine. As of 09/2025 it has over 150K stars on GitHub.

There are numerous other tools to run models locally:

* [llama.cpp](https://github.com/ggml-org/llama.cpp)
* [llamafile](https://github.com/Mozilla-Ocho/llamafile)
* [vllm](https://github.com/vllm-project/vllm) (used by various cloud services)
* [OpenLLM](https://github.com/bentoml/OpenLLM)

And even more user interfaces of various kinds.

As of 09/2025, of the [25809 repositories](https://github.com/topics/llm) on GitHub
tagged [llm], ollama seems to be among the top ten.





================================================
FILE: 10-Intro.md
================================================
# Intro

## Quick poll

* [ ] Who has already used Ollama?
* [ ] Who has run an LLM locally?
* [ ] Who has worked with model APIs before?

## Workshop goals

* [ ] Better understanding how Ollama works under the hood
* [ ] Learn about Model lifecycle and customization
* [ ] Build practical applications with embeddings in Go
* [ ] Create a visual search tool for handwritten notes

## Local and remote machine setup

* Install ollama on your laptop, you can follow the instructions here: [https://ollama.com/download](https://ollama.com/download)
* We will have access to a cloud GPU server instance, running a RTX 4000 SFF GPU with 20GB VRAM (an efficient GPU, that consumes at most 70W)

Most examples will work well on Linux and MacOS, but Windows WSL should also
work.

## Checkpoint: installation

```
$ ollama --version
$ ollama ls
$ curl localhost:11434
```

## Workshop material

All material available on
[github.com/miku/ollamaintro](https://github.com/miku/ollamaintro). Please
clone, as if contains also code examples and project scaffolding.





================================================
FILE: 20-Background.md
================================================
# Background

* interest in NLP since early 2000s,
  [wortschatz](https://wortschatz-leipzig.de/en)
([ex](https://dict.wortschatz-leipzig.de/en/res?corpusId=eng_news_2024&word=Uffizi+Gallery))
at Leipzig University

![Picture of a book: Dornseiff dictionary](static/9783110002874-de.jpg)

> 1933/34 unter dem Titel *Der deutsche Wortschatz synonymisch geordnet.* --
> [Dornseiff - Der deutsche Wortschatz nach
> Sachgruppen](https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/4961/file/Storjohann_Dornseiff_Der_deutsche_Wortschatz_nach_Sachgruppen_2012.pdf)

![](static/dornseiff-page.png)

Similar, popular, early project: [wordnet](https://en.wikipedia.org/wiki/WordNet):

> WordNet is a lexical database of semantic relations between words that links
> words into semantic relations including synonyms, hyponyms, and meronyms.

```sh
$ sudo apt install wordnet
$ wnb
```

![WordNet Browser Overview: go](static/screenshot-2025-09-20-170918-wordnet-go.png)

![WordNet Browser Hyponyms: train](static/screenshot-2025-09-20-170947-wordnet-train-hyponyms.png)

Statistical approach, counting words, sparse representation (bag-of-words), manual curation.

![](static/screenshot-2025-09-20-171719-gemini-norvig-quote.png)

Back then, there was an influencial paper about how more data wins over
algorithms, let me quickly ask an LLM which paper it was?

![](static/screenshot-2025-09-20-172118-claude-scaling-2001.png)

A few open weights LLMs struggle a bit with this question, but [Meta Llama 3.3
70B Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
(released December 6, 2024) seems to remember it.

![](static/screenshot-2025-09-20-172808-chatai-meta-llama3.3-70B-scaling-2001.png)

The paper was called: "Scaling to Very Very Large Corpora for Natural Language
Disambiguation" (2001)

> We collected a **1-billion-word** training corpus from a variety of English
> texts, including news articles, scientific abstracts, government transcripts,
> literature and other varied forms of prose. This training corpus is three
> orders of magnitude greater than the largest training corpus previously used
> for this problem.

You can generate 1B words in a few seconds today:

```
$ perf stat -r 1 -B ./1b > words.txt && du -h words.txt

 Performance counter stats for './1b':

          6,588.00 msec task-clock                       #    1.005 CPUs utilized
             4,216      context-switches                 #  639.951 /sec
                19      cpu-migrations                   #    2.884 /sec
               100      page-faults                      #   15.179 /sec
     6,480,220,375      cpu_atom/cycles/                 #    0.984 GHz                         (0.11%)
    24,934,230,875      cpu_core/cycles/                 #    3.785 GHz                         (99.45%)
     9,898,653,980      cpu_atom/instructions/           #    1.53  insn per cycle              (0.27%)
   122,415,155,922      cpu_core/instructions/           #    4.91  insn per cycle              (99.45%)
     2,911,671,826      cpu_atom/branches/               #  441.966 M/sec                       (0.35%)
    26,063,374,535      cpu_core/branches/               #    3.956 G/sec                       (99.45%)
        39,141,727      cpu_atom/branch-misses/          #    1.34% of all branches             (0.53%)
         4,678,785      cpu_core/branch-misses/          #    0.02% of all branches             (99.45%)
             TopdownL1 (cpu_core)                 #     10.6 %  tma_backend_bound
                                                  #      0.8 %  tma_bad_speculation
                                                  #     13.0 %  tma_frontend_bound
                                                  #     75.7 %  tma_retiring             (99.45%)
             TopdownL1 (cpu_atom)                 #     16.6 %  tma_bad_speculation
                                                  #     30.4 %  tma_retiring             (0.55%)
                                                  #     17.1 %  tma_backend_bound
                                                  #     35.9 %  tma_frontend_bound       (0.47%)

       6.556773774 seconds time elapsed

       5.171633000 seconds user
       1.444366000 seconds sys


4.7G    words.txt
```

## Some events and milestones

Cf. [arxiv sanity](https://github.com/karpathy/arxiv-sanity-preserver) (2016)

![](static/wayback-arxiv-sanity.png)

On the research side:

* 2003 "a neural probabilistic language model" (Y. Bengio et al.)
* 2013 word2vec (dense static vectors)
* 2014 fasttext (subword tokens)
* 2017 attention is all you need (no more recurrence, "transformer")
* 2018 BERT (comprehension) and original GPT-1 (generation)

...

> By the end of 2018, the field of NLP was about to undergo another
seismic change, marking the beginning of the era of foundation models -- [On
the Opportunities and Risks of Foundation
Models](https://arxiv.org/pdf/2108.07258)

* 2019 Language models are unsupervised multitask learners
* 2019 "bitter lesson"
* 2020 Language models are few shot learners
* 2020 "scaling laws"
* 2021 A General Language Assistant as a Laboratory for Alignment
* 2021 On the Opportunities and Risks of Foundation Models
* 2022 LLM are zero shot learners
* ...

List of open llms: [open-llms](https://github.com/eugeneyan/open-llms?tab=readme-ov-file#open-llms)

On the tooling/data side:

* 2015 tensorflow, keras
* 2016 pytorch
* 2018 jax, `pytorch_pretrained_bert` (later renamed HF transformers)
* 2020 "the pile" (800gb dataset)
* ...
* 2022-09-18 ggml (initial release)
* 2023-03-10 llama.cpp (initial release)
* 2023-07-20 ollama (initial release, support for two models)

And numerous more...

> Personal timeline:

* 2023-02-14, I ask a question on how long before we can run things locally at the [Leipzig Python User Group](https://lpug.github.io/) -- personally, I expected 2-5 years timeline
* 2023-04-18, we discuss C/GO and ggml (ai-on-the-edge) at [Leipzig Gophers #35](https://golangleipzig.space/posts/meetup-35-wrapup/)
* 2023-07-20, [ollama](https://ollama.ai) is released (with two models), [HN](https://news.ycombinator.com/item?id=36802582)
* 2023-11-21, 43 models (each with a couple of tags/versions)
* 2025-10-05, 100+ models, finetunes, language and multimodal models


## What makes local LLM more practical?

* quantization techniques, cf. [quantization](https://huggingface.co/docs/transformers/v4.56.2/quantization/overview)

> A research field, Quantization in deep learning, aim to reduce the high cost
> of computations and memory by representing the weights and activation in deep
> learning models with low precision data types. -- [A Comprehensive Study on Quantization Techniques for Large Language Models](https://arxiv.org/pdf/2411.02530v1)

![](static/model-size-increase-2017-2022.png)

From [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/pdf/2211.10438)

* efficient formats (GGUF)



================================================
FILE: 30-Architecture.md
================================================
[Empty file]


================================================
FILE: 32-Server.md
================================================
# Server

* implements API




================================================
FILE: 34-Runner.md
================================================
# Runner

Ollama serve subcommand starts an http server that coordinates requests and
spawns a subprocess per model to serve the actual requests.

This is the serve process and one model loaded from the blob store.

```
$ pstree -apT 143757
ollama,143757 serve
  â””â”€ollama,2720443 runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 --port 36431
```

We can run multiple models side by side, until we run out of RAM.

```
$ pstree -apT 143757
ollama,143757 serve
  â”œâ”€ollama,2720443 runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 --port 36431
  â”œâ”€ollama,3088461 runner --model /usr/share/ollama/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --port 38555
  â””â”€ollama,3089161 runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --port 43731
```

The runner process is a slim server:

```
$ /usr/bin/ollama runner --help
Runner usage
  -model string
        Path to model binary file
  -port int
        Port to expose the server on (default 8080)
  -verbose
        verbose output (default: disabled)
```

Example invocation:

```
$ /usr/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 --port 36432
```

## Background

Ollama started with a wrapper around llama and since 02/2025 (cf.
[#7913](https://github.com/ollama/ollama/pull/7913)) comes with an additional
runner type "ollama", that interfaces with ggml directly.

Currently two runner types:

* classic (cgo, llama.cpp)
* ollama (cgo, ggml)






================================================
FILE: 36-Tokenization.md
================================================
[Empty file]


================================================
FILE: 40-Model-Lifecycle.md
================================================
[Empty file]


================================================
FILE: 42-Model-Types.md
================================================
# Model Types

## Text generation

* most models

## Embedding models

* smaller models, with a single vector output; usually faster and can run on
  slower hardware as well

## Image understanding

* llava
* qwenvl

## Reasoning

* additional expansion of the user query intro a search tree



================================================
FILE: 44-Params.md
================================================
[Empty file]


================================================
FILE: 45-Templates.md
================================================
[Empty file]


================================================
FILE: 46-GGUF.md
================================================
# GGUF

## Overview

See also: [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

> GGUF is a file format for storing **models for inference with GGML** and
> executors based on GGML. GGUF is a binary format that is designed for fast
> loading and saving of models, and for ease of reading. Models are
> traditionally developed using PyTorch or another framework, and then
> converted to GGUF for use in GGML.

Some format evolution:

> GGML, GGMF, GGJT, and now GGUF â€” and it is now at **version three** of the format.

Full description: [gguf.md](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

## Highlights

* encodes both tensor data and metadata



================================================
FILE: 48-Modelfile.md
================================================
[Empty file]


================================================
FILE: 50-API.md
================================================
[Empty file]


================================================
FILE: 60-Request-Trace.md
================================================
# Tracing a request

Outline of the request flow for a completion request to the ollama api (coming
from some client).

## Request handling

* completion request arrives

## Runner instance

## Tokenization

## Batching



## Code Snippets

### Mostly llamarunner

```go
type CompletionRequest struct {
    Prompt  string
    Format  json.RawMessage
    Images  []ImageData
    Options *api.Options

    Grammar string // set before sending the request to the subprocess
}
```

A sequence is an abstraction over one request.

```go
type Sequence struct {
    // batch index
    iBatch int

    // number of tokens predicted so far
    numPredicted int

    // prompt inputs left to evaluate
    inputs []input

    // inputs that have been added to a batch but not yet submitted to Decode
    pendingInputs []input

    // tokens that have been generated but not returned yet (e.g. for stop sequences)
    pendingResponses []string

    // input cache being used by this sequence
    cache *InputCacheSlot

    // channel to send responses over
    responses chan string

    // channel to stop decoding (such as if the remote connection is closed)
    quit chan bool

    // number of tokens to predict
    numPredict int

    samplingCtx *llama.SamplingContext

    // channel to send back the embedding if embedding only
    embedding chan []float32

    // stop sequences
    stop []string

    // number of inputs to keep at the beginning when shifting context window
    numKeep int

    // true if an embedding are to be returned instead of text generation
    embeddingOnly bool

    doneReason llm.DoneReason

    // Metrics
    startProcessingTime time.Time
    startGenerationTime time.Time
    numDecoded          int
    numPromptInputs     int
}
```

The input can be a token or an image:

```
// input is an element of the prompt to process, either
// a token or an image embedding (generated from a vision projector)
type input struct {
    token int

    // embed is an image embedding
    embed []float32
}
```

The  server, on loading a model, allocates a maximum number of requests to
handle in parallel.

```go
        // runner/llamarunner/runner.go:807
        s.seqs = make([]*Sequence, s.parallel)
```

In llamarunner, the server keeps a number of slots for the sequences. The server will wait, until the a slow becomes available. Once it is available

```go
    // Ensure there is a place to put the sequence, released when removed from s.seqs
    if err := s.seqsSem.Acquire(r.Context(), 1); err != nil {
        if errors.Is(err, context.Canceled) {
            slog.Info("aborting completion request due to client closing the connection")
        } else {
            http.Error(w, fmt.Sprintf("Failed to acquire semaphore: %v", err), http.StatusInternalServerError)
        }
        return
    }

    s.mu.Lock()
    found := false
    for i, sq := range s.seqs {
        if sq == nil {
            seq.cache, seq.inputs, err = s.cache.LoadCacheSlot(seq.inputs, true)
            if err != nil {
                s.mu.Unlock()
                s.seqsSem.Release(1)
                http.Error(w, fmt.Sprintf("Failed to load cache: %v", err), http.StatusInternalServerError)
                return
            }

            s.seqs[i] = seq
            s.cond.Signal()
            found = true
            break
        }
    }
    s.mu.Unlock()
```

The server runs in a loop:

```
    for {
        select {
        case <-ctx.Done():
            return
        default:
            err := s.processBatch(tokenBatch, embedBatch)
            if err != nil {
                panic(err)
            }

            tokenBatch.Clear()
            embedBatch.Clear()
        }
    }
```

The server has a processBatch function; called from server run, which is started in a background thread.

```
    for range s.seqs {
        seqIdx = (seqIdx + 1) % len(s.seqs)
        seq := s.seqs[seqIdx]

        if seq == nil {
            continue
        }
```

In this processBatch function, we call `llama_decode` through the wrapper (cf.
[llama_context::decode](https://github.com/ggml-org/llama.cpp/blob/e6d65fb02d553bd79cad94e517cdca18b687788d/src/llama-context.cpp#L958-L1260),
which itself calls
[[llama_context::process_ubatch](https://github.com/ggml-org/llama.cpp/blob/e6d65fb02d553bd79cad94e517cdca18b687788d/src/llama-context.cpp#L732-L794))

There, we build the graph, set the inputs and run `graph_compute`, cf. [llama-context.cpp#L755-L784](https://github.com/ggml-org/llama.cpp/blob/e6d65fb02d553bd79cad94e517cdca18b687788d/src/llama-context.cpp#L775-L784)

```cpp
    // set the input data for the input tensors
    {
        //const auto t_start_us = ggml_time_us();

        res->set_inputs(&ubatch);

        //LLAMA_LOG_INFO("graph set inputs time: %.3f ms\n", (ggml_time_us() - t_start_us)/1000.0);
    }

    const auto status = graph_compute(res->get_gf(), ubatch.n_tokens > 1);
```


### New backend architecture

* new abstraction layer for backends (cf. `ml/backend.go`):

```go
type Backend interface {
    // Close frees all memory associated with this backend
    Close()

    Load(ctx context.Context, progress func(float32)) error

    // BackendMemory returns the memory allocations that were made for this model
    BackendMemory() BackendMemory

    Config() fs.Config
    Get(name string) Tensor
    NewContext() Context
    NewContextSize(size int) Context
}
```



================================================
FILE: 80-SimSearch.md
================================================
# Similarity Search

We are going to use an embedding model to cluster text.



================================================
FILE: 90-VisualGrep.md
================================================
[Empty file]


================================================
FILE: 99-Outro.md
================================================
[Empty file]


================================================
FILE: Refs.md
================================================
# References

* [https://tlbflush.org/post/2025_02_17_gguf_weekend/](https://tlbflush.org/post/2025_02_17_gguf_weekend/)



================================================
FILE: Slides.md
================================================
# Overview

* [Intro](10-Intro.md) [5]
* [Background](20-Background.md) [10]
* [Ollama architecture](30-Architecture.md)
    * [Server](32-Server.md) [5]
    * [Runner](34-Runner.md) [5]
    * [Tokenization](36-Tokenization.md) [5]
* [Model lifecycle](40-Model-Lifecycle.md)
    * [Model Types](42-Model-Types.md) [5]
    * [Model Parameters](44-Params.md) [5]
    * [Templates](45-Templates.md) [5]
    * [GGUF](46-GGUF.md) [5]
    * [Modelfile customizations](48-Modelfile.md) [5]
* [API](50-API.md) [15]
* [Tracing a request](60-Request-Trace.md) [5]
* [Ex: Similarity search](80-SimSearch.md) [25]
* [Ex: Visual grep](90-VisualGrep.md) [25]
* [Outro](99-Outro.md) [5]





================================================
FILE: attic/10-Intro.md
================================================
# Welcome

* A short disclaimer, first!

## Overview

We have a few parts:

* Background, [Intro](Intro.md)
* Ollama internals
* Ollama customizations
* Usage **example**: Similarity search
* Usage **example**: Grep through handwritten notes
* Wrap-up

## Background

* interest in NLP since early 2000s,
  [wortschatz](https://wortschatz-leipzig.de/en)
([ex](https://dict.wortschatz-leipzig.de/en/res?corpusId=eng_news_2024&word=Uffizi+Gallery))
at Leipzig University

![Picture of a book: Dornseiff dictionary](static/9783110002874-de.jpg)

> 1933/34 unter dem Titel *Der deutsche Wortschatz synonymisch geordnet.* --
> [Dornseiff - Der deutsche Wortschatz nach
> Sachgruppen](https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/4961/file/Storjohann_Dornseiff_Der_deutsche_Wortschatz_nach_Sachgruppen_2012.pdf)

![](static/dornseiff-page.png)

Similar, popular, early project: [wordnet](https://en.wikipedia.org/wiki/WordNet):

> WordNet is a lexical database of semantic relations between words that links
> words into semantic relations including synonyms, hyponyms, and meronyms.

```sh
$ sudo apt install wordnet
$ wnb
```

![WordNet Browser Overview: go](static/screenshot-2025-09-20-170918-wordnet-go.png)

![WordNet Browser Hyponyms: train](static/screenshot-2025-09-20-170947-wordnet-train-hyponyms.png)

Statistical approach, counting words, sparse representation (bag-of-words), manual curation.

![](static/screenshot-2025-09-20-171719-gemini-norvig-quote.png)

Back then, there was an influencial paper about how more data wins over
algorithms, let me quickly ask an LLM which paper it was?

![](static/screenshot-2025-09-20-172118-claude-scaling-2001.png)

A few open weights LLMs struggle a bit with this question, but [Meta Llama 3.3
70B Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
(released December 6, 2024) seems to remember it.

![](static/screenshot-2025-09-20-172808-chatai-meta-llama3.3-70B-scaling-2001.png)

The paper was called: "Scaling to Very Very Large Corpora for Natural Language
Disambiguation" (2001)

> We collected a **1-billion-word** training corpus from a variety of English
> texts, including news articles, scientific abstracts, government transcripts,
> literature and other varied forms of prose. This training corpus is three
> orders of magnitude greater than the largest training corpus previously used
> for this problem.

You can generate 1B words in a few seconds today:

```
$ perf stat -r 1 -B ./1b > words.txt && du -h words.txt

 Performance counter stats for './1b':

          6,588.00 msec task-clock                       #    1.005 CPUs utilized
             4,216      context-switches                 #  639.951 /sec
                19      cpu-migrations                   #    2.884 /sec
               100      page-faults                      #   15.179 /sec
     6,480,220,375      cpu_atom/cycles/                 #    0.984 GHz                         (0.11%)
    24,934,230,875      cpu_core/cycles/                 #    3.785 GHz                         (99.45%)
     9,898,653,980      cpu_atom/instructions/           #    1.53  insn per cycle              (0.27%)
   122,415,155,922      cpu_core/instructions/           #    4.91  insn per cycle              (99.45%)
     2,911,671,826      cpu_atom/branches/               #  441.966 M/sec                       (0.35%)
    26,063,374,535      cpu_core/branches/               #    3.956 G/sec                       (99.45%)
        39,141,727      cpu_atom/branch-misses/          #    1.34% of all branches             (0.53%)
         4,678,785      cpu_core/branch-misses/          #    0.02% of all branches             (99.45%)
             TopdownL1 (cpu_core)                 #     10.6 %  tma_backend_bound
                                                  #      0.8 %  tma_bad_speculation
                                                  #     13.0 %  tma_frontend_bound
                                                  #     75.7 %  tma_retiring             (99.45%)
             TopdownL1 (cpu_atom)                 #     16.6 %  tma_bad_speculation
                                                  #     30.4 %  tma_retiring             (0.55%)
                                                  #     17.1 %  tma_backend_bound
                                                  #     35.9 %  tma_frontend_bound       (0.47%)

       6.556773774 seconds time elapsed

       5.171633000 seconds user
       1.444366000 seconds sys


4.7G    words.txt
```

## Some events and milestones

Cf. [arxiv sanity](https://github.com/karpathy/arxiv-sanity-preserver) (2016)

![](static/wayback-arxiv-sanity.png)

On the research side:

* 2003 "a neural probabilistic language model" (Y. Bengio et al.)
* 2013 word2vec (dense static vectors)
* 2014 fasttext (subword tokens)
* 2017 attention is all you need (no more recurrence, "transformer")
* 2018 BERT (comprehension) and original GPT-1 (generation)

...

> By the end of 2018, the field of NLP was about to undergo another
seismic change, marking the beginning of the era of foundation models -- [On
the Opportunities and Risks of Foundation
Models](https://arxiv.org/pdf/2108.07258)

* 2019 Language models are unsupervised multitask learners
* 2019 "bitter lesson"
* 2020 Language models are few shot learners
* 2020 "scaling laws"
* 2021 A General Language Assistant as a Laboratory for Alignment
* 2021 On the Opportunities and Risks of Foundation Models
* 2022 LLM are zero shot learners
* ...

List of open llms: [open-llms](https://github.com/eugeneyan/open-llms?tab=readme-ov-file#open-llms)

On the tooling/data side:

* 2015 tensorflow, keras
* 2016 pytorch
* 2018 jax, `pytorch_pretrained_bert` (later renamed HF transformers)
* 2020 "the pile" (800gb dataset)
* ...
* 2022-09-18 ggml (initial release)
* 2023-03-10 llama.cpp (initial release)
* 2023-07-20 ollama (initial release, support for two models)

And numerous more...

> Personal discoveries:

* 2023-02-14, I ask a question on how long before we can run things locally at the [Leipzig Python User Group](https://lpug.github.io/) -- personally, I expected 2-5 years timeline
* 2023-04-18, we discuss C/GO and ggml (ai-on-the-edge) at [Leipzig Gophers #35](https://golangleipzig.space/posts/meetup-35-wrapup/)
* 2023-07-20, [ollama](https://ollama.ai) is released (with two models), [HN](https://news.ycombinator.com/item?id=36802582)
* 2023-11-21, today, 43 models (each with a couple of tags/versions)




================================================
FILE: attic/20-Internals.md
================================================
# Internals

## Overview

Ollama covers a few aspects of the LLM inference life cycle:

* manage **model life cycle**: download, store, customize, publish, remove
* **interactive chat**
* expose various **API** (native, openai compat)

## Inspired by docker

* ollama runs as a server, service
* cli makes request to the local (or some remote server)

## Ollama registry

The ollama project maintains a registry, which is modelled after docker. With a
bit of work, you can run your own registry (cf.
[#2388](https://github.com/ollama/ollama/issues/2388#issuecomment-1989307410)).

However, you can download files from other servers, too.

## Ollama model location

* model files reside under a dot folder (depending on platform)

```
$ tree -d /usr/share/ollama/.ollama/
/usr/share/ollama/.ollama/
â””â”€â”€ models
    â”œâ”€â”€ blobs
    â””â”€â”€ manifests
        â”œâ”€â”€ hf.co
        â”‚Â Â  â”œâ”€â”€ arcee-ai
        â”‚Â Â  â”‚Â Â  â””â”€â”€ SuperNova-Medius-GGUF
        â”‚Â Â  â”œâ”€â”€ nomic-ai
        â”‚Â Â  â”‚Â Â  â””â”€â”€ nomic-embed-text-v2-moe-gguf
        â”‚Â Â  â”œâ”€â”€ TheBloke
        â”‚Â Â  â”‚Â Â  â””â”€â”€ Mistral-7B-Instruct-v0.1-GGUF
        â”‚Â Â  â”œâ”€â”€ unsloth
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Nanonets-OCR-s-GGUF
        â”‚Â Â  â”‚Â Â  â””â”€â”€ Qwen3-Coder-30B-A3B-Instruct-GGUF
        â”‚Â Â  â””â”€â”€ xtuner
        â”‚Â Â      â””â”€â”€ llava-llama-3-8b-v1_1-gguf
        â””â”€â”€ registry.ollama.ai
            â””â”€â”€ library
                â”œâ”€â”€ all-minilm
                â”œâ”€â”€ bge-large
                â”œâ”€â”€ gemma2
                â”œâ”€â”€ gemma3
                â”œâ”€â”€ gemma3n
                â”œâ”€â”€ gpt-oss
                â”œâ”€â”€ granite3.2
                â”œâ”€â”€ granite3.2-vision
                â”œâ”€â”€ granite-embedding
                â”œâ”€â”€ llama3
                â”œâ”€â”€ llama3.2
                â”œâ”€â”€ llama3.2-vision
                â”œâ”€â”€ minicpm-v
                â”œâ”€â”€ mistral
                â”œâ”€â”€ mistral-nemo
                â”œâ”€â”€ mistral-small
                â”œâ”€â”€ mistral-small3.1
                â”œâ”€â”€ mistral-small3.2
                â”œâ”€â”€ moondream
                â”œâ”€â”€ mxbai-embed-large
                â”œâ”€â”€ nomic-embed-text
                â”œâ”€â”€ nomic-embed-text-v2
                â”œâ”€â”€ paraphrase-multilingual
                â”œâ”€â”€ qwen2.5-coder
                â”œâ”€â”€ qwen2.5vl
                â”œâ”€â”€ qwen3
                â”œâ”€â”€ qwen3-coder
                â”œâ”€â”€ smollm
                â””â”€â”€ smollm2

47 directories
```

We have manifests and blobs. Manifests link to blobs. You can download from
various model providers and also run your own. Default "registry.ollama.ai"
library contains official models (and variants people push/host there).

```
$ tree /usr/share/ollama/.ollama/models/manifests/ | head -40
/usr/share/ollama/.ollama/models/manifests/
â”œâ”€â”€ hf.co
â”‚Â Â  â”œâ”€â”€ arcee-ai
â”‚Â Â  â”‚Â Â  â””â”€â”€ SuperNova-Medius-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ Q8_0
â”‚Â Â  â”œâ”€â”€ nomic-ai
â”‚Â Â  â”‚Â Â  â””â”€â”€ nomic-embed-text-v2-moe-gguf
â”‚Â Â  â”‚Â Â      â””â”€â”€ latest
â”‚Â Â  â”œâ”€â”€ TheBloke
â”‚Â Â  â”‚Â Â  â””â”€â”€ Mistral-7B-Instruct-v0.1-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ latest
â”‚Â Â  â”œâ”€â”€ unsloth
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Nanonets-OCR-s-GGUF
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ BF16
â”‚Â Â  â”‚Â Â  â””â”€â”€ Qwen3-Coder-30B-A3B-Instruct-GGUF
â”‚Â Â  â”‚Â Â      â””â”€â”€ UD-Q4_K_XL
â”‚Â Â  â””â”€â”€ xtuner
â”‚Â Â      â””â”€â”€ llava-llama-3-8b-v1_1-gguf
â”‚Â Â          â””â”€â”€ latest
â””â”€â”€ registry.ollama.ai
    â””â”€â”€ library
        â”œâ”€â”€ all-minilm
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ bge-large
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gemma2
        â”‚Â Â  â””â”€â”€ 2b
        â”œâ”€â”€ gemma3
        â”‚Â Â  â”œâ”€â”€ 12b
        â”‚Â Â  â”œâ”€â”€ 1b
        â”‚Â Â  â”œâ”€â”€ 270m
        â”‚Â Â  â”œâ”€â”€ 27b
        â”‚Â Â  â”œâ”€â”€ 27b-it-qat
        â”‚Â Â  â”œâ”€â”€ 4b
        â”‚Â Â  â”œâ”€â”€ 4b-it-qat
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gemma3n
        â”‚Â Â  â””â”€â”€ latest
        â”œâ”€â”€ gpt-oss
        â”‚Â Â  â”œâ”€â”€ 20b
```

## Manifest file

A manifest groups the files that make up a model, including the raw weights, templates, license.

```json
$ cat /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library/gemma3n/latest | jq .
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "digest": "sha256:8eac5d7750c5bd24a9c556890ecb08ff749fefb7c4952b8962c5e7835aef21be",
    "size": 491
  },
  "layers": [
    {
      "mediaType": "application/vnd.ollama.image.model",
      "digest": "sha256:38e8dcc30df4eb0e29eaf5c74ba6ce3f2cd66badad50768fc14362acfb8b8cb6",
      "size": 7547579904
    },
    {
      "mediaType": "application/vnd.ollama.image.template",
      "digest": "sha256:e0a42594d802e5d31cdc786deb4823edb8adff66094d49de8fffe976d753e348",
      "size": 358
    },
    {
      "mediaType": "application/vnd.ollama.image.license",
      "digest": "sha256:1adbfec9dcf025cbf301c072f3847527468dcfa399da7491ee4a1c9e9f1b33e9",
      "size": 8363
    }
  ]
}
```

A sampling of the media types:

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[].mediaType' | sort | uniq -c | sort -nr

     45 application/vnd.ollama.image.model
     41 application/vnd.ollama.image.license
     36 application/vnd.ollama.image.params
     35 application/vnd.ollama.image.template
      8 application/vnd.ollama.image.system
      4 application/vnd.ollama.image.projector
```

### MediaType: model

The actual model, here in GGUF format. It includes metadata and the network layers.

See: [22-GGUF.md](22-GGUF.md)

### MediaType: licence

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[] | select(.mediaType == "application/vnd.ollama.image.license") | .digest'  | shuf -n 10

sha256:832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e
sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:dd084c7d92a3c1c14cc09ae77153b903fd2024b64a100a0cc8ec9316063d2dbc
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:cfc7749b96f63bd31c3c42b5c471bf756814053e847c10f3eb003417bc523d30
sha256:3e2c24001f9ef57bf7ec959a3658fbb49cdad113cdf394c264da9d16f9bdd132
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:a406579cd136771c705c521db86ca7d60a6f3de7c9b5460e6193a2df27861bde
sha256:2e68075caee4c571d43cb9d8d636415d8fe0fe59f695370cf56bc9f872f1ff3f
```

Example license:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e | head

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
```


### MediaType: params

```
$ find /usr/share/ollama/.ollama/models/manifests/registry.ollama.ai/library -type f | \
    xargs jq -rc '.layers[] | select(.mediaType == "application/vnd.ollama.image.params") | .digest'  | shuf -n 10

sha256:832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e
sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:dd084c7d92a3c1c14cc09ae77153b903fd2024b64a100a0cc8ec9316063d2dbc
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:cfc7749b96f63bd31c3c42b5c471bf756814053e847c10f3eb003417bc523d30
sha256:3e2c24001f9ef57bf7ec959a3658fbb49cdad113cdf394c264da9d16f9bdd132
sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1
sha256:a406579cd136771c705c521db86ca7d60a6f3de7c9b5460e6193a2df27861bde
sha256:2e68075caee4c571d43cb9d8d636415d8fe0fe59f695370cf56bc9f872f1ff3f
```

Example params:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-339e884a40f6708bc761d367f0c08e448d5bb6f16b3961c340e44e0e4835a004 | jq .
{
  "stop": [
    "<end_of_turn>"
  ],
  "top_k": 64,
  "top_p": 0.95
}

$ cat /usr/share/ollama/.ollama/models/blobs/sha256-e0daf17ff83eace4813f9e8554b262f6cc33ad880ff8df41a156ff9ef5522ddb | jq .
{
  "temperature": 0.15
}
```

Typical parameters:

```
$ ollama ls  | grep -v NAME | awk '{print $1}' | \
    xargs -I {} ollama show --parameters {} | sort  | awk '{print $1}' | uniq -c | sort -nr
     44 stop
     24 temperature
     15 top_p
     12 top_k
     12
      7 num_ctx
      5 repeat_penalty
      2 min_p
      1 num_keep
```


### MediaType: system

A system prompt.

```shell
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33

You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI,
a French startup headquartered in Paris.  You power an AI assistant called Le
Chat.  Your knowledge base was last updated on 2023-10-01.

When you're not sure about some information, you say that you don't have the
information and don't make up anything.  If the user's question is not clear,
ambiguous, or does not provide enough context for you to accurately answer the
question, you do not try to answer it right away and you rather ask the user to
clarify their request (e.g. "What are some good restaurants around me?" =>
"Where are you?" or "When is the next flight to Tokyo" => "Where do you travel
from?").  You are always very attentive to dates, in particular you try to
resolve dates (e.g. "yesterday" is {yesterday}) and when asked about
information at specific dates, you discard information that is at another date.
You follow these instructions in all languages, and always respond to the user
in the language they use or request.  Next sections describe the capabilities
that you have.

# WEB BROWSING INSTRUCTIONS

You cannot perform any web search or access internet to open URLs, links etc.
If it seems like the user is expecting you to do so, you clarify the situation
and ask the user to copy paste the text directly in the chat.

# MULTI-MODAL INSTRUCTIONS

You have the ability to read images, but you cannot generate images. You also
cannot transcribe audio files or videos.
```

Or just:

```
$ cat /usr/share/ollama/.ollama/models/blobs/sha256-75357d685f238b6afd7738be9786fdafde641eb6ca9a3be7471939715a68a4de
You are a helpful assistant.
```

### MediaType: projector

GGUF file, component for vision models.

```
ğŸ“‹ Model Metadata
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

General:
  general.description â”‚ image encoder for LLaVA
  general.name        â”‚ siglip-model
...
```


## Getting quick model info

Display model properties and parameters with `ollama show` subcommand:

```
$ ollama show gemma3:4b-it-qat
  Model
    architecture        gemma3
    parameters          4.3B
    context length      131072
    embedding length    2560
    quantization        Q4_0

  Capabilities
    completion
    vision

  Parameters
    top_p          0.95
    stop           "<end_of_turn>"
    temperature    1
    top_k          64

```

Extended help:






## Ollama Codebase

About 78K LOC Go. Ollama vendors [ggml](https://github.com/ggml-org/ggml) and
[llama.cpp](https://github.com/ggml-org/llama.cpp), which are C and C++ projects.

```
$ tokei
===============================================================================
 Language            Files        Lines         Code     Comments       Blanks
===============================================================================
 GNU Style Assembly      1            6            6            0            0
 Autoconf                1            4            4            0            0
 C                      12        26550        21425          947         4178
 C Header               87       122062       103618        11551         6893
 CMake                   8         1739         1403          112          224
 C++                    58        95684        75391         5559        14734
 C++ Header              3        26105        18285         4684         3136
 CSS                     1           34           29            0            5
 Dockerfile              1          123          104            3           16
 Go                    327        78674        64719         4230         9725
 HTML                    1            9            9            0            0
 JavaScript              2           13           12            1            0
 JSON                   38       147592       147592            0            0
 Objective-C             2         6816         5624          233          959
 PowerShell              2          265          242            9           14
 Protocol Buffers        1          333           98          178           57
 Shell                   7          608          476           39           93
 SVG                     1            9            9            0            0
 Plain Text              4       190212            0       167280        22932
 TSX                     2          129          122            0            7
 TypeScript              9          494          407           14           73
-------------------------------------------------------------------------------
 Markdown               25         3659            0         2439         1220
 |- BASH                 1            1            1            0            0
 |- Dockerfile           3           22           21            0            1
 |- Go                   1           23           22            0            1
 |- INI                  2           18           16            0            2
 |- JavaScript           1           43           34            1            8
 |- JSON                 1          504          504            0            0
 |- PowerShell           3            5            5            0            0
 |- Python               2           96           78            2           16
 |- Shell               15          629          624            0            5
 |- TypeScript           1           18           15            0            3
 (Total)                           5018         1320         2442         1256
===============================================================================
 Total                 593       701120       439575       197279        64266
===============================================================================
```


## High Level Request Flow

1. **Request Reception**: Client sends HTTP request to Ollama server (port
   11434) via CLI, REST API, or OpenAI-compatible endpoint

2. **Request Routing**: Server routes request through `server/routes.go` to
   appropriate handler (`/api/generate`, `/api/chat`, etc.)

3. **Model Management**: Scheduler (`server/sched.go`) checks if model is
   loaded; if not, loads model into memory with appropriate backend (CUDA/ROCm/Metal)

4. **Prompt Processing**: Request is parsed and prepared for inference by the
   runner system

5. **Inference Execution**: Specialized runner (`runner/`) performs model
   inference using optimized backends for token generation

6. **Response Streaming**: Generated tokens are streamed back to client via
   HTTP streaming (default mode) or returned as complete response

7. **Resource Management**: System manages GPU/CPU resources and model
   lifecycle for subsequent requests


## Model files



================================================
FILE: attic/21-Arch.md
================================================
# Ollama architecture

## Core ideas

* local execution (of models in GGUF format)
* model lifecycle (registry, customizations)
* various interfaces: chat, API

It is possible to get gguf files from any remote

## Client/Server

* server running as a service, exposing various API
* cli as API client
* easy to run or share a remote instance


```go
    // General
    r.HEAD("/", func(c *gin.Context) { c.String(http.StatusOK, "Ollama is running") })
    r.GET("/", func(c *gin.Context) { c.String(http.StatusOK, "Ollama is running") })
    r.HEAD("/api/version", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{"version": version.Version}) })
    r.GET("/api/version", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{"version": version.Version}) })

    // Local model cache management (new implementation is at end of function)
    r.POST("/api/pull", s.PullHandler)
    r.POST("/api/push", s.PushHandler)
    r.HEAD("/api/tags", s.ListHandler)
    r.GET("/api/tags", s.ListHandler)
    r.POST("/api/show", s.ShowHandler)
    r.DELETE("/api/delete", s.DeleteHandler)

    // Create
    r.POST("/api/create", s.CreateHandler)
    r.POST("/api/blobs/:digest", s.CreateBlobHandler)
    r.HEAD("/api/blobs/:digest", s.HeadBlobHandler)
    r.POST("/api/copy", s.CopyHandler)

    // Inference
    r.GET("/api/ps", s.PsHandler)
    r.POST("/api/generate", s.GenerateHandler)
    r.POST("/api/chat", s.ChatHandler)
    r.POST("/api/embed", s.EmbedHandler)
    r.POST("/api/embeddings", s.EmbeddingsHandler)

    // Inference (OpenAI compatibility)
    r.POST("/v1/chat/completions", openai.ChatMiddleware(), s.ChatHandler)
    r.POST("/v1/completions", openai.CompletionsMiddleware(), s.GenerateHandler)
    r.POST("/v1/embeddings", openai.EmbeddingsMiddleware(), s.EmbedHandler)
    r.GET("/v1/models", openai.ListMiddleware(), s.ListHandler)
    r.GET("/v1/models/:model", openai.RetrieveMiddleware(), s.ShowHandler)
```


## Path of a generate request

> Generate a response for a given prompt with a provided model. -- [docs](https://ollama.readthedocs.io/en/api/#generate-a-completion)


```go
// GenerateRequest describes a request sent by [Client.Generate]. While you
// have to specify the Model and Prompt fields, all the other fields have
// reasonable defaults for basic uses.
type GenerateRequest struct {
    // Model is the model name; it should be a name familiar to Ollama from
    // the library at https://ollama.com/library
    Model string `json:"model"`

    // Prompt is the textual prompt to send to the model.
    Prompt string `json:"prompt"`

    // Suffix is the text that comes after the inserted text.
    Suffix string `json:"suffix"`

    // System overrides the model's default system message/prompt.
    System string `json:"system"`

    // Template overrides the model's default prompt template.
    Template string `json:"template"`

    // Context is the context parameter returned from a previous call to
    // [Client.Generate]. It can be used to keep a short conversational memory.
    Context []int `json:"context,omitempty"`

    // Stream specifies whether the response is streaming; it is true by default.
    Stream *bool `json:"stream,omitempty"`

    // Raw set to true means that no formatting will be applied to the prompt.
    Raw bool `json:"raw,omitempty"`

    // Format specifies the format to return a response in.
    Format json.RawMessage `json:"format,omitempty"`

    // KeepAlive controls how long the model will stay loaded in memory following
    // this request.
    KeepAlive *Duration `json:"keep_alive,omitempty"`

    // Images is an optional list of raw image bytes accompanying this
    // request, for multimodal models.
    Images []ImageData `json:"images,omitempty"`

    // Options lists model-specific options. For example, temperature can be
    // set through this field, if the model supports it.
    Options map[string]any `json:"options"`

    // Think controls whether thinking/reasoning models will think before
    // responding. Can be a boolean (true/false) or a string ("high", "medium", "low")
    // for supported models. Needs to be a pointer so we can distinguish between false
    // (request that thinking _not_ be used) and unset (use the old behavior
    // before this option was introduced)
    Think *ThinkValue `json:"think,omitempty"`

    // DebugRenderOnly is a debug option that, when set to true, returns the rendered
    // template instead of calling the model.
    DebugRenderOnly bool `json:"_debug_render_only,omitempty"`
}
```

The completion model parses a model struct, hovering over the manifest.

```go
func GetModel(name string) (*Model, error) {
    mp := ParseModelPath(name)
    manifest, digest, err := GetManifest(mp)
    if err != nil {
        return nil, err
    }

    model := &Model{
        Name:      mp.GetFullTagname(),
        ShortName: mp.GetShortTagname(),
        Digest:    digest,
        Template:  template.DefaultTemplate,
    }

    if manifest.Config.Digest != "" {
        filename, err := GetBlobsPath(manifest.Config.Digest)
        if err != nil {
            return nil, err
        }

        configFile, err := os.Open(filename)
        if err != nil {
            return nil, err
        }
        defer configFile.Close()

        if err := json.NewDecoder(configFile).Decode(&model.Config); err != nil {
            return nil, err
        }
    }

    for _, layer := range manifest.Layers {
        filename, err := GetBlobsPath(layer.Digest)
        if err != nil {
            return nil, err
        }

        switch layer.MediaType {
        case "application/vnd.ollama.image.model":
            model.ModelPath = filename
            model.ParentModel = layer.From
        case "application/vnd.ollama.image.embed":
            // Deprecated in versions  > 0.1.2
            // TODO: remove this warning in a future version
            slog.Info("WARNING: model contains embeddings, but embeddings in modelfiles have been deprecated and will be ignored.")
        case "application/vnd.ollama.image.adapter":
            model.AdapterPaths = append(model.AdapterPaths, filename)
        case "application/vnd.ollama.image.projector":
            model.ProjectorPaths = append(model.ProjectorPaths, filename)
        case "application/vnd.ollama.image.prompt",
            "application/vnd.ollama.image.template":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }

            model.Template, err = template.Parse(string(bts))
            if err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.system":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }

            model.System = string(bts)
        case "application/vnd.ollama.image.params":
            params, err := os.Open(filename)
            if err != nil {
                return nil, err
            }
            defer params.Close()

            // parse model options parameters into a map so that we can see which fields have been specified explicitly
            if err = json.NewDecoder(params).Decode(&model.Options); err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.messages":
            msgs, err := os.Open(filename)
            if err != nil {
                return nil, err
            }
            defer msgs.Close()

            if err = json.NewDecoder(msgs).Decode(&model.Messages); err != nil {
                return nil, err
            }
        case "application/vnd.ollama.image.license":
            bts, err := os.ReadFile(filename)
            if err != nil {
                return nil, err
            }
            model.License = append(model.License, string(bts))
        }
    }

    return model, nil
}
```

This is more like model metadata:

```

type Model struct {
    Name           string `json:"name"`
    Config         ConfigV2
    ShortName      string
    ModelPath      string
    ParentModel    string
    AdapterPaths   []string
    ProjectorPaths []string
    System         string
    License        []string
    Digest         string
    Options        map[string]any
    Messages       []api.Message

    Template *template.Template
}
```

A first look into the request prompt, if it is empty we unload the model.

Some models have custom reponse renderers, like gptoss
[harmony](https://cookbook.openai.com/articles/openai-harmony).

Once the request input is validated, as runner is requested:

```
r, m, opts, err := s.scheduleRunner(c.Request.Context(), name.String(), caps, req.Options, req.KeepAlive)
```

Schedule runner will validated model capabilities, and will call `GetRunner`

```
    runnerCh, errCh := s.sched.GetRunner(ctx, model, opts, keepAlive)
```

The scheduler determines if the request could be satisfied by a loaded runner,
otherwise it is send to a pending requests queue.

When the model is available, we proceed to add images and preparing the prompt.

```go
  images := make([]llm.ImageData, len(req.Images))
    for i := range req.Images {
        images[i] = llm.ImageData{ID: i, Data: req.Images[i]}
    }
```

The server ask LlamaServer for completion.

```
        if err := r.Completion(c.Request.Context(), llm.CompletionRequest{
            Prompt:     prompt,
            Images:     images,
            Format:     req.Format,
            Options:    opts,
            UseHarmony: useHarmony,
        }, func(cr llm.CompletionResponse) {
            res := api.GenerateResponse{
                Model:     req.Model,
                CreatedAt: time.Now().UTC(),
                Response:  cr.Content,
                Done:      cr.Done,
                Thinking:  cr.Thinking,
                ToolCalls: cr.ToolCalls,
                Metrics: api.Metrics{
                    PromptEvalCount:    cr.PromptEvalCount,
                    PromptEvalDuration: cr.PromptEvalDuration,
                    EvalCount:          cr.EvalCount,
                    EvalDuration:       cr.EvalDuration,
                },
            }
```


## The runner process

Ollama has two runner options.

```
    if newRunner {
        return ollamarunner.Execute(args)
    } else {
        return llamarunner.Execute(args)
    }
```

## Input

```
// Input represents one token in the input stream
type Input struct {
    // Token is a single element of text.
    Token int32

    // Multimodal is represents a non-text element such as an
    // image (or part of one if the image can be processed in pieces).
    // It may be used either together with Token or on its own.
    Multimodal []Multimodal

    // MultimodalHash is a unique representation of the data
    // stored in Multimodal, used for caching and comparing
    // equality.
    MultimodalHash uint64

    // SameBatch forces the following number of tokens to be processed
    // in a single batch, breaking and extending batches as needed.
    // Useful for things like images that must be processed in one
    // shot.
    SameBatch int
}
```

## Batch

```
// Batch contains the inputs for a model forward pass
type Batch struct {
    // Inputs is the input tokens, including placeholders for multimodal inputs.
    Inputs ml.Tensor

    // Outputs are the set of indicies into Inputs for which output data should
    // be returned.
    Outputs ml.Tensor

    // Positions is the position for each Input, relative to its sequence. Equal
    // in length to Inputs.
    Positions []int32

    // Sequences is the sequence for each Input. Equal in length to Inputs.
    Sequences []int

    // Multimodal is a set of multimodal embeddings previously created by
    // EncodeMultimodal, along with an index into Inputs. Unused for text-only
    // models or for batches without multimodal elements.
    Multimodal []MultimodalIndex
}
```


## Loading and serving models

* models need to be loaded into memory
* diverse backends: CPU, GPU
* manage multiple models and requests (swapping models if required)





================================================
FILE: attic/22-GGUF.md
================================================
# GGUF

## Overview

See also: [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

> GGUF is a file format for storing **models for inference with GGML** and
> executors based on GGML. GGUF is a binary format that is designed for fast
> loading and saving of models, and for ease of reading. Models are
> traditionally developed using PyTorch or another framework, and then
> converted to GGUF for use in GGML.

Some format evolution:

> GGML, GGMF, GGJT, and now GGUF â€” and it is now at **version three** of the format.

Full description: [gguf.md](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)

## Format overview

* magic
* version
* number of tensors
* number of key value pairs (metadata)

![](static/313174776-c3623641-3a1d-408e-bfaf-1b7c4e16aa63.png)

## Naming convention

```
<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf
```

Examples:

```
Mixtral-8x7B-v0.1-KQ2.gguf
Hermes-2-Pro-Llama-3-8B-F16.gguf
Grok-100B-v1.0-Q4_0-00003-of-00009.gguf
```

## readgguf

Example cli application: readgguf. You can point it to a file or URL to get
some information about gguf file.

Random, example project:
[SmolDocling-256M-preview-GGUF](https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF),
[files and
versions](https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF/tree/main)

```
$ curl -sLo SmolDocling-256M-preview-q8_0.gguf "https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF/resolve/main/SmolDocling-256M-preview-q8_0.gguf?download=true"
```

## Quantization

* [Blind testing different quants](https://github.com/ggml-org/llama.cpp/discussions/5962)



================================================
FILE: attic/25-Eval.md
================================================
# Evaluation

Leaderboards, e.g.

* [https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro)

Example test:

* [Ollama-MMLU](https://github.com/chigkim/Ollama-MMLU-Pro/)

Adapted from [MMLU-Pro](https://github.com/TIGER-AI-Lab/MMLU-Pro)

> We introduce MMLU-Pro, an enhanced benchmark designed to evaluate language
> understanding models across broader and more challenging tasks. Building on
> the Massive Multitask Language Understanding (MMLU) dataset, MMLU-Pro
> integrates more challenging, reasoning-focused questions and increases the
> answer choices per question from four to ten, significantly raising the
> difficulty and reducing the chance of success through random guessing.
> MMLU-Pro comprises over 12,000 rigorously curated questions from academic
> exams and textbooks, spanning 14 diverse domains including Biology, Business,
> Chemistry, Computer Science, Economics, Engineering, Health, History, Law,
> Math, Philosophy, Physics, Psychology, and Others.

## Issues

* isolated tests
* training on test cases (for a random note, see various comments on r/locallama, e.g. [n6hg2rc](https://www.reddit.com/r/LocalLLaMA/comments/1mfitwb/comment/n6hg2rc/))



================================================
FILE: attic/25-Tokenization.md
================================================
[Empty file]


================================================
FILE: attic/30-Customizations.md
================================================
[Empty file]


================================================
FILE: attic/40-SimilaritySearch.md
================================================
[Empty file]


================================================
FILE: attic/50-Vgrep.md
================================================
[Empty file]


================================================
FILE: attic/60-Wrapup.md
================================================
[Empty file]


================================================
FILE: extra/ModelFormats.md
================================================
# Model Formats

* pytorch, pth
* tensorflow, protobuf

Typically full precision models.

* ONNX (Open Neural Network Exchange) is a common shared format
* GGUF, focus on loading, quantization




================================================
FILE: extra/Quantization.md
================================================
# Quantization Notes

* quanto; pytorch model
* model compression

Options:

* model pruning (removing layers that have little effect on performance)
* knowledge distillation (student model; teacher model)

You can quantize weights, or activations.

Common example: FP32 to INT8; using 25% of the storage; aim: reduce
quantization error

## Example

We take float values and map them to integers. Sampling is a form of
quantization. Smaller models, faster inference.

* DeepSeek-R1 is about 720GB in 671B parameters

## Cycle

* training usually in float32, float16 (bfloat16), or even float8
* inference: only forward path; int8, int4, int1 (1 bit)

> PTQ, post training quantization.

Quantization aware training. Training tweaks to make a model more resilient to
a loss in precision.





================================================
FILE: x/seq/README.md
================================================
# Sequence

May need to install deps first:

> Linux

```shell
# Install nlohmann-json
sudo apt-get update
sudo apt-get install nlohmann-json3-dev

# Install miniaudio (might need to install manually)
# Download miniaudio.h from https://github.com/mackron/miniaudio
# Or install via package manager if available
sudo apt-get install libminiaudio-dev
```

> MacOS

```shell
# Using Homebrew
brew install nlohmann-json
brew install miniaudio

# If you don't have Homebrew, install it first:
# /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```


